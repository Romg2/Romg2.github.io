---
title: "[Python] 모두의 딥러닝 - 03. 신경망의 이해[딥러닝 문제와 해결]"
excerpt: "딥러닝의 문제와 해결: 활성화 함수, 확률적 경사하강법"
categories: 
  - DL_all
tags: 
    - Python
    - DL_all
toc: TRUE
toc_sticky: TRUE
use_math: TRUE
---


**모두의 딥러닝** 교재를 토대로 공부한 내용입니다.

실습과정에서 필요에 따라 코드나 이론에 대한 추가, 수정사항이 있습니다.

---

**기본 세팅**


```python
import numpy as np
import pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
```


```python
%matplotlib inline
%config InlineBackend.figure_format = 'retina'

mpl.rc('font', family='NanumGothic') # 폰트 설정
mpl.rc('axes', unicode_minus=False) # 유니코드에서 음수 부호 설정

# 차트 스타일 설정
sns.set(font="NanumGothic", rc={"axes.unicode_minus":False}, style='darkgrid')
plt.rc("figure", figsize=(10,8))

warnings.filterwarnings("ignore")
```

---

# 9. 딥러닝 문제와 해결

앞서 [오차 역전파 포스팅](https://romg2.github.io/dl_all/02-%EB%AA%A8%EB%91%90%EC%9D%98-%EB%94%A5%EB%9F%AC%EB%8B%9D-03.-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EC%9D%B4%ED%95%B4-(2)/)에서 오차 역전파를 이용해 가중치와 바이어스를 구하는 방법을 배웠다.

오차 역전파를 이용해서 다층 퍼셉트론은 신경망이 되었고 신경망을 쌓아서 인공지능을 완성할 수 있을 것이다.

그런데 생각보다 결과가 좋아지지 않았다고 하는데 그 이유와 해결법에 대해 알아보자.

(참고로 이번 포스팅에는 코드는 하나도 없다.)

## 9.1 활성화 함수

### 9.1.1 문제점

오차 역전파는 출력층으로부터 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정하는 방법이다.

가중치를 수정하기 위해선 기울기(오차의 미분 값)가 필요하다.

그런데 층이 늘어나면서 역전파를 통해 전달되는 기울기의 값이 점점 작아져 맨 처음 층에 전달되지 않는 문제가 발생하였다.

이 문제를 기울기 소실이라고 한다.

그렇다면 기울기 소실 문제가 왜 발생하였을까?

이는 시그모이드 함수의 특성 때문인데 다음 그림을 보면 시그모이드 함수를 미분하였을 때 최대값이 0.3이다.

1보다 작기에 계속 곱하다 보면 0에 가까워져 여러 층을 거칠수록 기울기가 사라져 가중치 수정이 어려워졌다.

![](https://thebook.io/img/080228/121_2.jpg)

출처: <https://thebook.io/080228/part03/ch09/01-01/>

### 9.1.2 해결법

이를 해결하기 위해서 다양한 활성화 함수를 사용하였다.

![](https://thebook.io/img/080228/122.jpg)

출처: <https://thebook.io/080228/part03/ch09/01-01/>

- 하이퍼볼릭 탄젠트: 시그모이드보다 미분 값의 범위가 확장 되었지만 여전히 1보다 작으므로 기울기 소실 문제가 발생한다.


- 렐루: $x$가 0보다 크면 항상 1이므로 여러 은닉층을 거치며 곱해지더라도 처음 층까지 사라지지 않고 남아있을 수 있다.


- 소프트플러스: 렐루의 미분 값이 0이 되는 순간을 완화한 함수

## 9.2 경사하강법

### 9.2.1 문제점

경사하강법은 정확하게 가중치를 찾아가지만, 업데이트마다 전체 데이터를 미분해야하므로 계산량이 매우 많다.

이로 인해 속도가 느려지며, 최적 해를 찾기 전에 최적화 과정을 멈출 수도 있다.

### 9.2.2 해결법

**[확률적 경사하강법]**

전체 데이터가 아닌 랜덤하게 추출한 일부 데이터를 사용하는 방법이다.

경사하강법에 비해 중간 결과의 진폭이 크긴 하지만 속도가 훨씬 빠르며 최적 해에 근사하므로 대안으로 사용된다.

다음 그림은 경사하강법과 확률적 경사하강법의 차이를 나타낸 그림이다.

![](https://thebook.io/img/080228/123.jpg)

출처: <https://thebook.io/080228/part03/ch09/02/>

머신러닝 완벽가이드를 공부하면서 [경사하강법 포스팅](https://romg2.github.io/mlguide/07_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C-05.-%ED%9A%8C%EA%B7%80-%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95/)에 확률적 경사하강법(SGD)에 대해 정리하였으니 참고하자.

**[모멘텀 SGD]**

모멘텀은 관성, 탄력, 가속도라는 의미로 모멘텀 SGD는 말 그대로 경사하강법에 탄력을 더해준다.

경사하강법과 마찬가지로 매번 기울기를 구하지만, 

오차를 수정하기 전 바로 앞 수정 값과 방향(+,-)을 고려하여 같은 방향으로 일정한 비율만 수정되게 하는 방법이다.

따라서 양수(+)방향, 음수(-) 방향으로 지그재그로 이동하는 현상이 줄어든다.

또한 이전 이동 값을 고려해서 다음 값을 결정하기에 관성의 효과를 얻을 수 있다.

다음 그림은 확률적 경사하강법과 모멘텀을 적용한 확률적 경사하강법의 차이를 나타낸 그림이다.

![](https://thebook.io/img/080228/124.jpg)

출처: <https://thebook.io/080228/part03/ch09/02-01/>
