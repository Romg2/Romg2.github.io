---
title: 이미지/마크다운 테스트
categories: 
  - dss
tags: 데이터 사이언스 스쿨
toc: TRUE
toc_sticky: TRUE
---

오류 마크다운
$
x_i =
\begin{bmatrix}
x_{i1} \\ x_{i2} \\ \vdots \\ x_{in}
\end{bmatrix}
$

$
\newline
\rightarrow
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{\sigma^{2}((X^{T}X)^{-1})_{ii}}} \div
\sqrt{\dfrac{MSE}{\sigma^{2}}} 
\ = 
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{MSE((X^{T}X)^{-1})_{ii}}}
\sim T_{(n-k)}
$

$\hat{y}_{i} = \sum_{j}h_{ij}y_{j}$

수정 버전

$
x_i =
\begin{bmatrix}
x_{i1}\\x_{i2}\\\vdots\\x_{in}
\end{bmatrix}
$

\begin{bmatrix}1&2\\3&4\\ \end{bmatrix}

$
\rightarrow
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{\sigma^{2}((X^{T}X)^{-1})_{ii}}} \div
\sqrt{\dfrac{MSE}{\sigma^{2}}} 
\ = 
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{MSE((X^{T}X)^{-1})_{ii}}}
\sim T_{(n-k)}
$

$$\hat{y}_{i} = \sum_j h_{ij}y_{j}$$
$$\hat{y}_{i} = \sum_j {h_{ij}y_{j}}$$
$$\hat{y} = \sum_{j}^{d} h_{ij}y_{j}$$



```python
import numpy as np
import pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

from io import StringIO
import missingno as msno
from sklearn.impute import SimpleImputer

from patsy import demo_data
from patsy import dmatrix
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import FunctionTransformer

import scipy as sp
import statsmodels.api as sm
from statsmodels.stats.stattools import durbin_watson

import warnings
```


```python
%matplotlib inline
%config InlineBackend.figure_format = 'retina'

mpl.rc('font', family='NanumGothic') # 폰트 설정
mpl.rc('axes', unicode_minus=False) # 유니코드에서 음수 부호 설정

# 차트 스타일 설정
sns.set(font="NanumGothic", rc={"axes.unicode_minus":False}, style='darkgrid')
plt.rc("figure", figsize=(10,8))

warnings.filterwarnings("ignore")
```

# 4.1 회귀분석 예제

## 4.1.1 보스턴 집값 예측

sklearn 패키지의 datasets 서브패키지는 회귀분석을 공부하기 위한 예제를 제공한다. 그 중 `load_boston()` 명령으로 받을 수 있는 보스턴 주택 가격 데이터는 다음과 같은 데이터이다. 보스턴의 506개 타운(town)의 13개 독립변수값로부터 해당 타운의 주택가격 중앙값을 예측하는 문제다. 사용할 수 있는 특징 데이터는 다음과 같다.

* 독립변수
 * CRIM: 범죄율
 * INDUS: 비소매상업지역 면적 비율
 * NOX: 일산화질소 농도 
 * RM: 주택당 방 수
 * LSTAT: 인구 중 하위 계층 비율
 * B: 인구 중 흑인 비율
 * PTRATIO: 학생/교사 비율
 * ZN: 25,000 평방피트를 초과 거주지역 비율
 * CHAS: 찰스강의 경계에 위치한 경우는 1, 아니면 0
 * AGE: 1940년 이전에 건축된 주택의 비율
 * RAD: 방사형 고속도로까지의 거리
 * DIS: 직업센터의 거리
 * TAX:	재산세율


* 종속변수
 * 보스턴 506개 타운의 1978년 주택 가격 중앙값 (단위 1,000 달러)

`load_boston` 명령으로 받는 데이터 집합은 `Bunch` 라는 클래스 객체로 생성된다. 이 클래스 객체는 다음과 같은 속성을 가진다.


 * `data`: 독립변수 `ndarray` 배열
 * `target`: 종속변수 `ndarray` 배열
 * `feature_names`: 독립변수 이름 리스트
 * `target_names`: (옵션) 종속변수 이름 리스트
 * `DESCR`: (옵션) 자료에 대한 설명 문자열

**Boston 데이터 불러오기**


```python
from sklearn.datasets import load_boston

temp = load_boston()
dir(temp)
```




    ['DESCR', 'data', 'feature_names', 'filename', 'target']




```python
x_data = pd.DataFrame(temp.data, columns = temp.feature_names)
y_data = pd.DataFrame(temp.target, columns = ["MEDV"])

boston = pd.concat([x_data, y_data], axis = 1)
boston.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>MEDV</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.03237</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>6.998</td>
      <td>45.8</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>394.63</td>
      <td>2.94</td>
      <td>33.4</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.06905</td>
      <td>0.0</td>
      <td>2.18</td>
      <td>0.0</td>
      <td>0.458</td>
      <td>7.147</td>
      <td>54.2</td>
      <td>6.0622</td>
      <td>3.0</td>
      <td>222.0</td>
      <td>18.7</td>
      <td>396.90</td>
      <td>5.33</td>
      <td>36.2</td>
    </tr>
  </tbody>
</table>
</div>



**일부 변수 시각화**


```python
sns.pairplot(boston[["MEDV", "RM", "AGE"]])
plt.suptitle("Boston Pair Plot, 일부 독립변수와 종속변수 시각화", y=1.03)
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_11_0.png)
    


- 방 개수(RM)가 증가할 수록 집값(MEDV)은 증가하는 경향이 보인다.


- 노후화 정도(AGE)와 집값은 관계(MEDV)가 없어 보인다.

## 4.1.2 당뇨병 진행도 예측

scikit-learn 패키지가 제공하는 당뇨병 진행도 예측용 데이터는 442명의 당뇨병 환자를 대상으로한 검사 결과를 나타내는 데이터이다.

* 독립변수 [모두 스케일링(scaling)되어있음.]

 * age: 나이

 * sex: 성별

 * bmi: BMI(Body mass index)지수

 * bp: 평균혈압

 * s1~s6: 6종류의 혈액검사수치
 

* 종속변수
 * 1년 뒤 측정한 당뇨병의 진행률

**Diabetes 데이터 불러오기**


```python
from sklearn.datasets import load_diabetes

temp = load_diabetes()
diabetes = pd.DataFrame(temp.data, columns=temp.feature_names)
diabetes["target"] = temp.target
diabetes.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>age</th>
      <th>sex</th>
      <th>bmi</th>
      <th>bp</th>
      <th>s1</th>
      <th>s2</th>
      <th>s3</th>
      <th>s4</th>
      <th>s5</th>
      <th>s6</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>437</th>
      <td>0.041708</td>
      <td>0.050680</td>
      <td>0.019662</td>
      <td>0.059744</td>
      <td>-0.005697</td>
      <td>-0.002566</td>
      <td>-0.028674</td>
      <td>-0.002592</td>
      <td>0.031193</td>
      <td>0.007207</td>
      <td>178.0</td>
    </tr>
    <tr>
      <th>438</th>
      <td>-0.005515</td>
      <td>0.050680</td>
      <td>-0.015906</td>
      <td>-0.067642</td>
      <td>0.049341</td>
      <td>0.079165</td>
      <td>-0.028674</td>
      <td>0.034309</td>
      <td>-0.018118</td>
      <td>0.044485</td>
      <td>104.0</td>
    </tr>
    <tr>
      <th>439</th>
      <td>0.041708</td>
      <td>0.050680</td>
      <td>-0.015906</td>
      <td>0.017282</td>
      <td>-0.037344</td>
      <td>-0.013840</td>
      <td>-0.024993</td>
      <td>-0.011080</td>
      <td>-0.046879</td>
      <td>0.015491</td>
      <td>132.0</td>
    </tr>
    <tr>
      <th>440</th>
      <td>-0.045472</td>
      <td>-0.044642</td>
      <td>0.039062</td>
      <td>0.001215</td>
      <td>0.016318</td>
      <td>0.015283</td>
      <td>-0.028674</td>
      <td>0.026560</td>
      <td>0.044528</td>
      <td>-0.025930</td>
      <td>220.0</td>
    </tr>
    <tr>
      <th>441</th>
      <td>-0.045472</td>
      <td>-0.044642</td>
      <td>-0.073030</td>
      <td>-0.081414</td>
      <td>0.083740</td>
      <td>0.027809</td>
      <td>0.173816</td>
      <td>-0.039493</td>
      <td>-0.004220</td>
      <td>0.003064</td>
      <td>57.0</td>
    </tr>
  </tbody>
</table>
</div>



**일부 변수 시각화**


```python
sns.pairplot(diabetes[["target", "bmi", "bp", "s1"]])
plt.suptitle("Diabetes Pair Plot, 일부 독립변수와 종속변수 시각화", y=1.03)
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_19_0.png)
    


- 독립변수 BMI지수와 평균혈압은 종속변수인 당뇨병 진행도와 양의 상관관계를 가지는 것을 볼 수 있다. 


- 독립변수 BMI지수와 평균혈압도 서로 양의 상관관계를 가져 다중공선성이 보인다.

## 4.1.3 연습문제

sklearn.datasets 패키지의 fetch_california_housing 명령은 캘리포니아 주택가격을 예측하기위한 데이터다. 이 데이터의 독립변수를 조사하고 어떤 데이터들이 주택가격과 상관관계가 있는지를 조사한다. 또한 서로 강한 상관관계를 가지는 독립변수도 알아보자.

**California 데이터 불러오기**


```python
from sklearn.datasets import fetch_california_housing

temp = fetch_california_housing()

california = pd.DataFrame(temp.data, columns=temp.feature_names)
california["MedHouseVal"] = temp.target
california.tail()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
      <th>MedHouseVal</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>20635</th>
      <td>1.5603</td>
      <td>25.0</td>
      <td>5.045455</td>
      <td>1.133333</td>
      <td>845.0</td>
      <td>2.560606</td>
      <td>39.48</td>
      <td>-121.09</td>
      <td>0.781</td>
    </tr>
    <tr>
      <th>20636</th>
      <td>2.5568</td>
      <td>18.0</td>
      <td>6.114035</td>
      <td>1.315789</td>
      <td>356.0</td>
      <td>3.122807</td>
      <td>39.49</td>
      <td>-121.21</td>
      <td>0.771</td>
    </tr>
    <tr>
      <th>20637</th>
      <td>1.7000</td>
      <td>17.0</td>
      <td>5.205543</td>
      <td>1.120092</td>
      <td>1007.0</td>
      <td>2.325635</td>
      <td>39.43</td>
      <td>-121.22</td>
      <td>0.923</td>
    </tr>
    <tr>
      <th>20638</th>
      <td>1.8672</td>
      <td>18.0</td>
      <td>5.329513</td>
      <td>1.171920</td>
      <td>741.0</td>
      <td>2.123209</td>
      <td>39.43</td>
      <td>-121.32</td>
      <td>0.847</td>
    </tr>
    <tr>
      <th>20639</th>
      <td>2.3886</td>
      <td>16.0</td>
      <td>5.254717</td>
      <td>1.162264</td>
      <td>1387.0</td>
      <td>2.616981</td>
      <td>39.37</td>
      <td>-121.24</td>
      <td>0.894</td>
    </tr>
  </tbody>
</table>
</div>



**상관관계 확인**


```python
corr_M = california.corr()
mask = np.array(corr_M)
mask[np.tril_indices_from(mask)] = False

plt.figure(figsize= (10,8))

sns.heatmap(corr_M, 
            cmap = sns.light_palette("red", as_cmap=True),
            annot = True, 
            mask = mask)

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_26_0.png)
    


- 종속변수 MedHouseVal와 상관관계가 높은 독립변수는 MedInc이다.


- 독립변수간에는 AveRooms와 AveBedrms, Latitude와 Longitude가 서로 강한 상관관계를 가지고 있다. 

**일부 변수 시각화**


```python
sns.pairplot(california[["MedHouseVal", "MedInc", "AveRooms", "AveBedrms", "Latitude", "Longitude"]])
plt.suptitle("California Pair Plot, 일부 독립변수와 종속변수 시각화", y=1.03)
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_29_0.png)
    


- 상관계수로 확인 하였듯이 변수간 선형관계가 보인다.


- Latitude와 Longitude는 비선형 형태에 가까워 보인다.

## 4.1.4 가상 데이터 예측

**가상 데이터 만들기**
```python
X, Y, B  = make_regression(
    n_samples, n_features, bias, noise, random_state, coef=True, n_informative, effective_rank, tail_strength)
```

 * n_samples: 정수 (옵션, 디폴트 100)
    * 표본 데이터의 갯수 n
    
 
 * n_features: 정수 (옵션, 디폴트 100)
    * 독립변수(feature)의 수(차원) p
    
    
 * bias: 실수 (옵션, 디폴트 0)
    * Y 절편
 
 
 * noise: 실수 (옵션, 디폴트 0)
    * 오차항의 표준편차 $\epsilon$의 표준편차
 
 
 * random_state: 정수 (옵션, 디폴트 None)
   * 난수 발생용 시드값 
 
 
 * coef: 불리언 (옵션, 디폴트 False)
   * True 이면 선형 모형의 계수도 출력
 
 
 * n_informative: 정수 (옵션, 디폴트 10)
     * 독립변수(feature) 중 실제로 종속변수와 상관 관계가 있는 독립변수의 수(차원)
 
 
 * effective_rank: 정수 또는 None (옵션, 디폴트 None)
     * 독립변수(feature) 중 서로 독립인 독립변수의 수. 만약 None이면 모두 독립
 
 
 * tail_strength: 0부터 1사이의 실수 (옵션, 디폴트 0.5)
     * effective_rank가 None이 아닌 경우 독립변수간의 상관관계를 결정하는 변수. 0.5면 독립변수간의 상관관계가 없다.
     
출력은 다음과 같다.

 * X: (n_samples, n_features) 형상의 2차원 배열
    * 독립변수의 표본 데이터 행렬 $X$, n x p
 
 
 * Y: (n_samples) 형상의 1차원 배열
    * 종속변수의 표본 데이터 벡터 $Y$, n x 1
 
 
 * B: (n_features) 형상의 1차원 배열 또는 (n_features, n_targets) 형상의 2차원 배열 (옵션)
    * 선형 모형의 계수 벡터 $\beta$, p x 1 입력 인수 coef가 True 인 경우에만 출력됨 - 절편 제외


```python
from sklearn.datasets import make_regression

X, Y, B = make_regression(
    n_samples=200, n_features=1, bias=2, noise=10, coef=True, random_state=1017
)
```


```python
x = np.linspace(-3, 3, 100)
y0 = B * x + 2

plt.figure(figsize= (10,8))

# 회귀식 - 오차항 고려없이 계수로 생성, 모수 beta를 이미 알고 있음
plt.plot(x, y0, "r-")

# 산점도
plt.scatter(X, Y)

plt.xlabel("x")
plt.ylabel("y")
plt.title("make_regression 예제")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_34_0.png)
    


- `make_regression` 으로 만든 가상 데이터 산점도이다.


```python
def make_regression2(n_informative, effective_rank, tail_strength, i):
    X, Y, B = make_regression(
        n_samples=300, n_features=2, noise=10, coef=True, random_state=0,
        n_informative = n_informative, effective_rank= effective_rank, tail_strength = tail_strength
    )
    
    axs[i].scatter(X[:, 0], X[:, 1], c=Y, s=100, cmap=mpl.cm.bone)

    axs[i].set_xlabel("x1")
    axs[i].set_ylabel("x2")

fig, axs = plt.subplots(1,3, figsize= (15,6))

make_regression2(2,None,0.5, 0)
make_regression2(1,None,0.5, 1)
make_regression2(2,1,0, 2)
    
plt.axis("equal")

axs[0].set_title("독립변수: 서로 독립, 종속변수와 모두 상관 관계")
axs[1].set_title("독립변수: 서로 독립, 종속변수와 하나만 상관 관계")
axs[2].set_title("독립변수가 독립이 아닌 경우")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_36_0.png)
    


- 종속변수 값은 점의 명암으로 표시하였으며 밝을수록 종속변수 값이 큰 것이고 점의 어두울수록 작은 값이다.


- 첫 번째 산점도에선 x1, x2 모두 증가할수록 종속변수 값이 커지는 것을 알 수 있다.


- 두 번째 산점도에선 x2가 증가할수록 종속변수 값이 증가하지만 x2에 따라선 큰 차이가 없다.


- 세 번쨰 산점도에선 x1, x2가 상관관계를 가지는 것을 확인 할 수 있다.

# 4.2 선형회귀분석의 기초

$n$개의 관측값을 가지는 $i$번째 독립변수 $x_i$는 다음과 같이 벡터로 표현가능하다. [$n$ x $1$]

$
x_i =
\begin{bmatrix}
x_{i1} \\ x_{i2} \\ \vdots \\ x_{in}
\end{bmatrix}
$

$n$개의 관측값을 가지고 $k-1$개의 독립변수를 가지는 행렬 $X$는 다음과 같다. [절편 포함: $n$ x $k$] 

$
X =
\begin{bmatrix}
1 & x_1 & x_2 & \cdots & x_p
\end{bmatrix}
\ = 
\begin{bmatrix}
1 & x_{11} & x_{21} & \cdots & x_{(k-1)1} \\
1 & x_{12} & x_{22} & \cdots & x_{(k-1)2} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_{1n} & x_{2n} & \cdots & x_{(k-1)n} \\
\end{bmatrix}
$

회귀계수 $\beta$ [$k$ x 1], 오차 $\epsilon$ [$n$ x 1], 종속변수 $Y$ [$n$ x 1]는 다음과 같다. 

$
\beta =
\begin{bmatrix}
\beta_{0} \\ \beta_{1} \\ \vdots \\ \beta_{p}
\end{bmatrix}
, \quad 
\epsilon =
\begin{bmatrix}
\epsilon_{0} \\ \epsilon_{1} \\ \vdots \\ \epsilon_{n}
\end{bmatrix}
, \quad 
Y =
\begin{bmatrix}
y_{0} \\ y_{1} \\ \vdots \\ y_{n}
\end{bmatrix}
= X\beta + \epsilon
$

- 만약 $X^{T}X$ 행렬의 역행렬이 존재한다면 추정회귀계수 $\hat{\beta}$ 은 다음과 같다.


- $\hat{\beta} = (X^{T}X)^{-1}X^{T}Y$


- $X^{T}X$ 행렬의 역행렬이 존재하기 위해선 각 독립변수가 서로 독립이어야 한다.

## 4.2.1 Numpy를 이용한 선형회귀분석


```python
from sklearn.datasets import make_regression

# n=200, p=1, epsilon~N(0,sqrt(10))
X0, Y, B = make_regression(
    n_samples=200, n_features=1, bias=100, noise=10, coef=True, random_state=1
)

X = sm.add_constant(X0) # 상수항 결합
Y = Y.reshape(len(Y), 1)

# 모회귀계수
print("회귀계수 베타0:", 100)
print("회귀계수 베타1:", B.round(3))
print("-" * 30)

# 추정회귀계수
B_hat = np.linalg.inv(X.T @ X) @ X.T @ Y

print("추정회귀계수 베타0:", B_hat[0,0].round(3))
print("추정회귀계수 베타1:", B_hat[1,0].round(3))
```

    회귀계수 베타0: 100
    회귀계수 베타1: 86.448
    ------------------------------
    추정회귀계수 베타0: 99.792
    추정회귀계수 베타1: 86.962
    

- 실제 회귀계수와 비슷한 값으로 추정한 것을 확인 할 수 있다.


```python
x_new = np.linspace(np.min(X), np.max(X), 10)
X_new = sm.add_constant(x_new)
y_new = np.dot(X_new, B_hat) # 예측값

plt.figure(figsize= (10,8))

plt.scatter(X0, Y, label="실제 데이터")
plt.plot(x_new, y_new, 'rs-', label="추정회귀식")

plt.xlabel("x")
plt.ylabel("y")
plt.title("선형 회귀분석의 예")
plt.legend()

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_44_0.png)
    


## 4.2.2 scikit-learn 패키지를 사용한 선형 회귀분석


```python
from sklearn.linear_model import LinearRegression

model = LinearRegression(fit_intercept=True).fit(X0,Y)

print("추정회귀계수 베타0:", model.intercept_[0].round(3))
print("추정회귀계수 베타1:", model.coef_[0][0].round(3))
```

    추정회귀계수 베타0: 99.792
    추정회귀계수 베타1: 86.962
    

- `LinearRegression` 을 이용해 `Numpy`에서 수기로 계산한 값과 같은 결과를 얻었다.


```python
model.predict([[-2], [-1], [0], [1], [2]])
```




    array([[-74.13191534],
           [ 12.82979668],
           [ 99.79150869],
           [186.7532207 ],
           [273.71493272]])



- `predict` 를 이용해 새로운 x값에 대해 추정값을 구할 수 있다.


- 이때 x는 2차원 배열로 기입해야한다.

## 4.2.3 statsmodels 패키지를 사용한 선형 회귀분석


```python
data = pd.DataFrame({"x": X0[:, 0], "y": Y[:, 0]})
data
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.232495</td>
      <td>127.879017</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.038696</td>
      <td>93.032914</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.550537</td>
      <td>161.857508</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.503185</td>
      <td>141.692050</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.186980</td>
      <td>283.260119</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>195</th>
      <td>-0.172428</td>
      <td>87.874277</td>
    </tr>
    <tr>
      <th>196</th>
      <td>-1.199268</td>
      <td>-13.626664</td>
    </tr>
    <tr>
      <th>197</th>
      <td>1.462108</td>
      <td>216.106619</td>
    </tr>
    <tr>
      <th>198</th>
      <td>1.131629</td>
      <td>212.743149</td>
    </tr>
    <tr>
      <th>199</th>
      <td>0.495211</td>
      <td>150.017589</td>
    </tr>
  </tbody>
</table>
<p>200 rows × 2 columns</p>
</div>



**독립변수와 종속변수가 다른 데이터프레임에 존재하는 경우**


```python
# x에 상수항을 수기로 추가해주어야 한다.
dfy = data[["y"]]
dfX = sm.add_constant(data[["x"]])

model = sm.OLS(dfy, dfX)
result = model.fit()
print(result.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                      y   R-squared:                       0.985
    Model:                            OLS   Adj. R-squared:                  0.985
    Method:                 Least Squares   F-statistic:                 1.278e+04
    Date:                Fri, 21 May 2021   Prob (F-statistic):          8.17e-182
    Time:                        17:17:51   Log-Likelihood:                -741.28
    No. Observations:                 200   AIC:                             1487.
    Df Residuals:                     198   BIC:                             1493.
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const         99.7915      0.705    141.592      0.000      98.402     101.181
    x             86.9617      0.769    113.058      0.000      85.445      88.479
    ==============================================================================
    Omnibus:                        1.418   Durbin-Watson:                   1.690
    Prob(Omnibus):                  0.492   Jarque-Bera (JB):                1.059
    Skew:                           0.121   Prob(JB):                        0.589
    Kurtosis:                       3.262   Cond. No.                         1.16
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    

- 같은 회귀계수가 출력된 것을 확인 가능하며 결정계수 등 다양한 정보가 출력된다.

**독립변수와 종속변수가 같은 데이터프레임에 존재하는 경우**


```python
model = sm.OLS.from_formula("y ~ x", data = data)
result = model.fit()
print(result.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                      y   R-squared:                       0.985
    Model:                            OLS   Adj. R-squared:                  0.985
    Method:                 Least Squares   F-statistic:                 1.278e+04
    Date:                Fri, 21 May 2021   Prob (F-statistic):          8.17e-182
    Time:                        17:17:51   Log-Likelihood:                -741.28
    No. Observations:                 200   AIC:                             1487.
    Df Residuals:                     198   BIC:                             1493.
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept     99.7915      0.705    141.592      0.000      98.402     101.181
    x             86.9617      0.769    113.058      0.000      85.445      88.479
    ==============================================================================
    Omnibus:                        1.418   Durbin-Watson:                   1.690
    Prob(Omnibus):                  0.492   Jarque-Bera (JB):                1.059
    Skew:                           0.121   Prob(JB):                        0.589
    Kurtosis:                       3.262   Cond. No.                         1.16
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    

- 동일한 결과로 출력된다.


```python
result.predict({"x": [-2, -1, 0, 1, 2] })
```




    0    -74.131915
    1     12.829797
    2     99.791509
    3    186.753221
    4    273.714933
    dtype: float64



- `predict` 를 이용해 새로운 x값에 대해 추정값을 구할 수 있다.


- `predict` 외의 `params`, `resid` 등 다양한 메소드가 내제되어있다.

# 4.3 스케일링

## 4.3.1 회귀분석과 조건수

**회귀분석에서 조건수가 커지는 경우**

1. 변수들의 단위 차이로 인해 숫자의 스케일이 크게 달라지는 경우. 이 경우에는 스케일링(scaling)으로 해결한다.


2. 다중 공선성 즉, 상관관계가 큰 독립 변수들이 있는 경우, 이 경우에는 변수 선택이나 PCA를 사용한 차원 축소 등으로 해결한다.

**Boston 데이터 회귀분석**


```python
from sklearn.datasets import load_boston

boston = load_boston()

dfX = pd.DataFrame(boston.data, columns=boston.feature_names)
dfy = pd.DataFrame(boston.target, columns=["MEDV"])
df = pd.concat([dfX, dfy], axis=1)

model1 = sm.OLS.from_formula("MEDV ~ " + "+".join(boston.feature_names), data=df)
result1 = model1.fit()
print(result1.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.741
    Model:                            OLS   Adj. R-squared:                  0.734
    Method:                 Least Squares   F-statistic:                     108.1
    Date:                Fri, 21 May 2021   Prob (F-statistic):          6.72e-135
    Time:                        17:17:51   Log-Likelihood:                -1498.8
    No. Observations:                 506   AIC:                             3026.
    Df Residuals:                     492   BIC:                             3085.
    Df Model:                          13                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept     36.4595      5.103      7.144      0.000      26.432      46.487
    CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043
    ZN             0.0464      0.014      3.382      0.001       0.019       0.073
    INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141
    CHAS           2.6867      0.862      3.118      0.002       0.994       4.380
    NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262
    RM             3.8099      0.418      9.116      0.000       2.989       4.631
    AGE            0.0007      0.013      0.052      0.958      -0.025       0.027
    DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084
    RAD            0.3060      0.066      4.613      0.000       0.176       0.436
    TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005
    PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696
    B              0.0093      0.003      3.467      0.001       0.004       0.015
    LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425
    ==============================================================================
    Omnibus:                      178.041   Durbin-Watson:                   1.078
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
    Skew:                           1.521   Prob(JB):                    8.84e-171
    Kurtosis:                       8.281   Cond. No.                     1.51e+04
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 1.51e+04. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

- Boston 데이터 회귀분석 결과 Notes[2]를 해석하면 다음과 같다.

    조건수(conditiona number)가 15100으로 너무 큽니다.
        
    강한 다중공선성(multicollinearity)이나 다른 수치적 문제가 있을 수 있습니다.


```python
dfX.std()
```




    CRIM         8.601545
    ZN          23.322453
    INDUS        6.860353
    CHAS         0.253994
    NOX          0.115878
    RM           0.702617
    AGE         28.148861
    DIS          2.105710
    RAD          8.707259
    TAX        168.537116
    PTRATIO      2.164946
    B           91.294864
    LSTAT        7.141062
    dtype: float64



- Boston 데이터의 독립변수의 경우 단위가 0.1 수준부터 100 수준까지 혼재되어 있다.


```python
feature_names = list(boston.feature_names)
feature_names.remove("CHAS") 
feature_names = [f"scale({name})" for name in feature_names] + ["CHAS"]

model3 = sm.OLS.from_formula("MEDV ~ " + "+".join(feature_names), data=df)
result3 = model3.fit()

print(result3.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.741
    Model:                            OLS   Adj. R-squared:                  0.734
    Method:                 Least Squares   F-statistic:                     108.1
    Date:                Fri, 21 May 2021   Prob (F-statistic):          6.72e-135
    Time:                        17:17:51   Log-Likelihood:                -1498.8
    No. Observations:                 506   AIC:                             3026.
    Df Residuals:                     492   BIC:                             3085.
    Df Model:                          13                                         
    Covariance Type:            nonrobust                                         
    ==================================================================================
                         coef    std err          t      P>|t|      [0.025      0.975]
    ----------------------------------------------------------------------------------
    Intercept         22.3470      0.219    101.943      0.000      21.916      22.778
    scale(CRIM)       -0.9281      0.282     -3.287      0.001      -1.483      -0.373
    scale(ZN)          1.0816      0.320      3.382      0.001       0.453       1.710
    scale(INDUS)       0.1409      0.421      0.334      0.738      -0.687       0.969
    scale(NOX)        -2.0567      0.442     -4.651      0.000      -2.926      -1.188
    scale(RM)          2.6742      0.293      9.116      0.000       2.098       3.251
    scale(AGE)         0.0195      0.371      0.052      0.958      -0.710       0.749
    scale(DIS)        -3.1040      0.420     -7.398      0.000      -3.928      -2.280
    scale(RAD)         2.6622      0.577      4.613      0.000       1.528       3.796
    scale(TAX)        -2.0768      0.633     -3.280      0.001      -3.321      -0.833
    scale(PTRATIO)    -2.0606      0.283     -7.283      0.000      -2.617      -1.505
    scale(B)           0.8493      0.245      3.467      0.001       0.368       1.331
    scale(LSTAT)      -3.7436      0.362    -10.347      0.000      -4.454      -3.033
    CHAS               2.6867      0.862      3.118      0.002       0.994       4.380
    ==============================================================================
    Omnibus:                      178.041   Durbin-Watson:                   1.078
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
    Skew:                           1.521   Prob(JB):                    8.84e-171
    Kurtosis:                       8.281   Cond. No.                         10.6
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    

- 스케일링 하였을 때 조건수가 10.6으로 줄어든 것을 확인 할 수 있다.

# 4.4 범주형 독립변수

## 4.4.1 축소랭크(Reduce-Rank)


```python
from sklearn.datasets import load_boston

boston = load_boston()

dfX = pd.DataFrame(boston.data, columns=boston.feature_names)
dfy = pd.DataFrame(boston.target, columns=["MEDV"])
df_boston = pd.concat([dfX, dfy], axis=1)

model1 = sm.OLS.from_formula("MEDV ~ " + "+".join(boston.feature_names), data=df_boston)
result1 = model1.fit()
print(result1.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.741
    Model:                            OLS   Adj. R-squared:                  0.734
    Method:                 Least Squares   F-statistic:                     108.1
    Date:                Fri, 21 May 2021   Prob (F-statistic):          6.72e-135
    Time:                        17:17:51   Log-Likelihood:                -1498.8
    No. Observations:                 506   AIC:                             3026.
    Df Residuals:                     492   BIC:                             3085.
    Df Model:                          13                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept     36.4595      5.103      7.144      0.000      26.432      46.487
    CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043
    ZN             0.0464      0.014      3.382      0.001       0.019       0.073
    INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141
    CHAS           2.6867      0.862      3.118      0.002       0.994       4.380
    NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262
    RM             3.8099      0.418      9.116      0.000       2.989       4.631
    AGE            0.0007      0.013      0.052      0.958      -0.025       0.027
    DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084
    RAD            0.3060      0.066      4.613      0.000       0.176       0.436
    TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005
    PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696
    B              0.0093      0.003      3.467      0.001       0.004       0.015
    LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425
    ==============================================================================
    Omnibus:                      178.041   Durbin-Watson:                   1.078
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
    Skew:                           1.521   Prob(JB):                    8.84e-171
    Kurtosis:                       8.281   Cond. No.                     1.51e+04
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 1.51e+04. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

- 0과 1로 구성된 범주형 변수 CHAS는 축소랭크방식으로 회귀 계수가 구해졌다.


- CHAS가 0인 경우 Intercept는 36.4595, CHAS가 1인 경우 Intercept는 39.1462가 된다.


- CHAS의 회귀계수 2.6867은 0을 기준으로 1일 때 절편이 얼마나 커지는지를 의미한다.

**두 개 이상의 범주형 변수가 있는 경우**

- 축소랭크 방식을 이용한다.

## 4.4.2 풀랭크(Full-Rank)


```python
feature_names = list(boston.feature_names)
feature_names.remove("CHAS") 
feature_names = [name for name in feature_names] + ["C(CHAS)"]
model2 = sm.OLS.from_formula("MEDV ~ 0 + " + "+".join(feature_names), data=df_boston)
result2 = model2.fit()
print(result2.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.741
    Model:                            OLS   Adj. R-squared:                  0.734
    Method:                 Least Squares   F-statistic:                     108.1
    Date:                Fri, 21 May 2021   Prob (F-statistic):          6.72e-135
    Time:                        17:17:51   Log-Likelihood:                -1498.8
    No. Observations:                 506   AIC:                             3026.
    Df Residuals:                     492   BIC:                             3085.
    Df Model:                          13                                         
    Covariance Type:            nonrobust                                         
    ================================================================================
                       coef    std err          t      P>|t|      [0.025      0.975]
    --------------------------------------------------------------------------------
    C(CHAS)[0.0]    36.4595      5.103      7.144      0.000      26.432      46.487
    C(CHAS)[1.0]    39.1462      5.153      7.597      0.000      29.023      49.270
    CRIM            -0.1080      0.033     -3.287      0.001      -0.173      -0.043
    ZN               0.0464      0.014      3.382      0.001       0.019       0.073
    INDUS            0.0206      0.061      0.334      0.738      -0.100       0.141
    NOX            -17.7666      3.820     -4.651      0.000     -25.272     -10.262
    RM               3.8099      0.418      9.116      0.000       2.989       4.631
    AGE              0.0007      0.013      0.052      0.958      -0.025       0.027
    DIS             -1.4756      0.199     -7.398      0.000      -1.867      -1.084
    RAD              0.3060      0.066      4.613      0.000       0.176       0.436
    TAX             -0.0123      0.004     -3.280      0.001      -0.020      -0.005
    PTRATIO         -0.9527      0.131     -7.283      0.000      -1.210      -0.696
    B                0.0093      0.003      3.467      0.001       0.004       0.015
    LSTAT           -0.5248      0.051    -10.347      0.000      -0.624      -0.425
    ==============================================================================
    Omnibus:                      178.041   Durbin-Watson:                   1.078
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
    Skew:                           1.521   Prob(JB):                    8.84e-171
    Kurtosis:                       8.281   Cond. No.                     2.01e+04
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 2.01e+04. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

- 풀랭크 방식을 사용했을 때는 각각의 절편값으로 출력되는 것을 확인할 수 있다.

## 4.4.3 범주형 독립변수와 실수 독립변수의 상호작용

**절편이 같고 기울기가 다른 모형**


```python
model3 = sm.OLS.from_formula("MEDV ~ CRIM + C(CHAS):CRIM", data=df_boston)
result3 = model3.fit()
print(result3.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.174
    Model:                            OLS   Adj. R-squared:                  0.171
    Method:                 Least Squares   F-statistic:                     53.15
    Date:                Fri, 21 May 2021   Prob (F-statistic):           1.15e-21
    Time:                        17:17:51   Log-Likelihood:                -1791.7
    No. Observations:                 506   AIC:                             3589.
    Df Residuals:                     503   BIC:                             3602.
    Df Model:                           2                                         
    Covariance Type:            nonrobust                                         
    =======================================================================================
                              coef    std err          t      P>|t|      [0.025      0.975]
    ---------------------------------------------------------------------------------------
    Intercept              23.8231      0.408     58.452      0.000      23.022      24.624
    CRIM                   -0.4198      0.043     -9.688      0.000      -0.505      -0.335
    C(CHAS)[T.1.0]:CRIM     1.7699      0.466      3.799      0.000       0.854       2.685
    ==============================================================================
    Omnibus:                      121.360   Durbin-Watson:                   0.751
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              231.021
    Skew:                           1.351   Prob(JB):                     6.83e-51
    Kurtosis:                       4.912   Cond. No.                         12.0
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    

- CHAS가 0일 때 CRIM의 기울기는 -0.4198이다.


- CHAS가 1일 때 CRIM의 기울기는 1.7699 - 0.4198 = 1.3501이다.


- 절편은 23.8231로 CHAS와 상관없이 동일하다.

**절편과 기울기 모두 다른 모형**


```python
model4 = sm.OLS.from_formula("MEDV ~ C(CHAS) + CRIM + C(CHAS):CRIM", data=df_boston)
result4 = model4.fit()
print(result4.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.181
    Model:                            OLS   Adj. R-squared:                  0.176
    Method:                 Least Squares   F-statistic:                     36.87
    Date:                Fri, 21 May 2021   Prob (F-statistic):           1.52e-21
    Time:                        17:17:51   Log-Likelihood:                -1789.9
    No. Observations:                 506   AIC:                             3588.
    Df Residuals:                     502   BIC:                             3605.
    Df Model:                           3                                         
    Covariance Type:            nonrobust                                         
    =======================================================================================
                              coef    std err          t      P>|t|      [0.025      0.975]
    ---------------------------------------------------------------------------------------
    Intercept              23.6377      0.418     56.595      0.000      22.817      24.458
    C(CHAS)[T.1.0]          3.5036      1.816      1.930      0.054      -0.064       7.071
    CRIM                   -0.4123      0.043     -9.502      0.000      -0.498      -0.327
    C(CHAS)[T.1.0]:CRIM     1.1137      0.576      1.934      0.054      -0.018       2.245
    ==============================================================================
    Omnibus:                      120.101   Durbin-Watson:                   0.767
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              226.558
    Skew:                           1.343   Prob(JB):                     6.36e-50
    Kurtosis:                       4.879   Cond. No.                         46.5
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    

- CHAS가 0일 때 절편은 23.6377, CRIM의 기울기는 -0.4123이다.


- CHAS가 1일 때 절편은 23.6377 + 3.5036 = 27.1413, CRIM의 기울기는 1.1137 - 0.4123 = 0.7014이다.

# 4.5 부분회귀

## 4.5.1 부분회귀 플롯

독립변수의 갯수가 많을 때 특정한 하나의 독립변수의 영향력을 시각화하는 방법


```python
from sklearn.datasets import load_boston

boston = load_boston()

dfX0 = pd.DataFrame(boston.data, columns=boston.feature_names)
dfX = sm.add_constant(dfX0) # 상수항 추가
dfy = pd.DataFrame(boston.target, columns=["MEDV"])
df = pd.concat([dfX, dfy], axis=1)

model_boston = sm.OLS(dfy, dfX)
result_boston = model_boston.fit()
```

**단순 시각화**


```python
plt.figure(figsize= (10,8))

sns.regplot(x="AGE", y="MEDV", data=df)
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_92_0.png)
    


- AGE와 MEDV의 산점도를 보면 음의 상관관계가 있어 보인다.

**부분회귀 플롯**

`plot_partregress(endog, exog_i, exog_others, data=None, obs_labels=True, ret_coords=False)`

- endog: 종속변수 문자열


- exog_i: 분석 대상이 되는 독립변수 문자열


- exog_others: 나머지 독립변수 문자열의 리스트


- data: 모든 데이터가 있는 데이터프레임


- obs_labels: 데이터 라벨링 여부


- ret_coords: 잔차 데이터 반환 여부


```python
# 나머지 독립변수 (상수항 포함)
others = list(set(df.columns).difference(set(["MEDV", "AGE"])))

p, resids = sm.graphics.plot_partregress(
    "MEDV", "AGE", others, data=df, obs_labels=False, ret_coords=True
)

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_96_0.png)
    


- x축: AGE를 종속변수, 원래 종속변수인 MEDV를 제외한 나머지 변수를 독립변수로 하였을 때 잔차


- y축: MEDV를 종속변수, AGE를 제외한 나머지 변수를 독립변수로 하였을 때 잔차


- 부분회귀 플롯으로 보았을 때 AGE와 MEDV간의 상관관계가 없음을 알 수 있다.

**전체 부분회귀 플롯**

`plot_partregress_grid(result, fig)`

- result: 회귀분석 결과 객체


- fig: plt.figure 객체


```python
fig = plt.figure(figsize=(10, 20))
sm.graphics.plot_partregress_grid(result_boston, fig=fig)
fig.suptitle("")
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_100_0.png)
    


## 4.5.2 CCPR

**CCPR(Component-Component plus Residual)**

`plot_ccpr(result, exog_idx)`

- result: 회귀분석 결과 객체


- exog_idx: 분석 대상이 되는 독립변수 문자열


```python
plt.rc("figure", figsize=(10,8))

sm.graphics.plot_ccpr(result_boston, "AGE")
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_104_0.png)
    


- x축: AGE


- y축: 기존 회귀적합모형의 잔차 + $\hat{\beta}AGE$

**전체 CCPR**

`plot_ccpr_grid(result, fig)`

- result: 회귀분석 결과 객체


- fig: plt.figure 객체


```python
fig = plt.figure(figsize=(10, 20))
sm.graphics.plot_ccpr_grid(result_boston, fig=fig)
fig.suptitle("")
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_108_0.png)
    


**부분회귀 플롯 + CCPR**

`plot_regress_exog(result, exog_idx)`

- result: 회귀분석 결과 객체


- exog_idx: 분석 대상이 되는 독립변수 문자열


```python
fig = sm.graphics.plot_regress_exog(result_boston, "AGE")
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_111_0.png)
    


# 4.6 확률론적 선형 회귀모형

**모형구조**

$
\epsilon \sim \ N(0,\sigma^{2}) , \quad Y = X\beta + \epsilon
\;
\rightarrow Y \sim \ N(X\beta,\sigma^{2})
$

$\hat{\beta} \sim \ MN(\beta, \ \sigma^{2}(X^{T}X)^{-1})$


$
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{\sigma^{2}((X^{T}X)^{-1})_{ii}}} \sim \ Z,
\quad
\sqrt{\dfrac{(n-k)MSE}{\sigma^{2}}}
\ = 
\sqrt{\dfrac{RSS}{\sigma^{2}}} \sim \ \chi^{2}(n-k)
$

$
\newline
\rightarrow
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{\sigma^{2}((X^{T}X)^{-1})_{ii}}} \div
\sqrt{\dfrac{MSE}{\sigma^{2}}} 
\ = 
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{MSE((X^{T}X)^{-1})_{ii}}}
\sim T_{(n-k)}
$

- 오차항은 정규성, 독립성, 등분산성등을 고려해 가정한다.


- 종속변수는 선형회귀모형으로서 모수인 회귀계수가 주어졌을 때 정규분포를 따르게 된다.


- 추정회귀계수도 종속변수의 선형결합 형태이므로 정규분포를 따르나 실제로 모분산을 알 수 없으므로 검정에선 MSE를 사용하여 T분포를 사용한다.

## 4.6.1 회귀계수의 검정


```python
from sklearn.datasets import load_boston

boston = load_boston()

dfX = pd.DataFrame(boston.data, columns=boston.feature_names)
dfy = pd.DataFrame(boston.target, columns=["MEDV"])
df_boston = pd.concat([dfX, dfy], axis=1)

feature_names = list(boston.feature_names)
feature_names.remove("CHAS") 
feature_names = [name for name in feature_names] + ["C(CHAS)"]

model2 = sm.OLS.from_formula("MEDV ~ 0 + " + "+".join(feature_names), data=df_boston)
result2 = model2.fit()

print(result2.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.741
    Model:                            OLS   Adj. R-squared:                  0.734
    Method:                 Least Squares   F-statistic:                     108.1
    Date:                Fri, 21 May 2021   Prob (F-statistic):          6.72e-135
    Time:                        17:17:58   Log-Likelihood:                -1498.8
    No. Observations:                 506   AIC:                             3026.
    Df Residuals:                     492   BIC:                             3085.
    Df Model:                          13                                         
    Covariance Type:            nonrobust                                         
    ================================================================================
                       coef    std err          t      P>|t|      [0.025      0.975]
    --------------------------------------------------------------------------------
    C(CHAS)[0.0]    36.4595      5.103      7.144      0.000      26.432      46.487
    C(CHAS)[1.0]    39.1462      5.153      7.597      0.000      29.023      49.270
    CRIM            -0.1080      0.033     -3.287      0.001      -0.173      -0.043
    ZN               0.0464      0.014      3.382      0.001       0.019       0.073
    INDUS            0.0206      0.061      0.334      0.738      -0.100       0.141
    NOX            -17.7666      3.820     -4.651      0.000     -25.272     -10.262
    RM               3.8099      0.418      9.116      0.000       2.989       4.631
    AGE              0.0007      0.013      0.052      0.958      -0.025       0.027
    DIS             -1.4756      0.199     -7.398      0.000      -1.867      -1.084
    RAD              0.3060      0.066      4.613      0.000       0.176       0.436
    TAX             -0.0123      0.004     -3.280      0.001      -0.020      -0.005
    PTRATIO         -0.9527      0.131     -7.283      0.000      -1.210      -0.696
    B                0.0093      0.003      3.467      0.001       0.004       0.015
    LSTAT           -0.5248      0.051    -10.347      0.000      -0.624      -0.425
    ==============================================================================
    Omnibus:                      178.041   Durbin-Watson:                   1.078
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
    Skew:                           1.521   Prob(JB):                    8.84e-171
    Kurtosis:                       8.281   Cond. No.                     2.01e+04
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 2.01e+04. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

- 기본적으로 회귀분석 결과에 각 회귀계수가 0인지에 대해 p-value 및 신뢰구간이 나타난다.

**단일계수 검정**


```python
print(result2.t_test("CRIM = -0.1"))
```

                                 Test for Constraints                             
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    c0            -0.1080      0.033     -0.244      0.808      -0.173      -0.043
    ==============================================================================
    

- p-value가 0.808로서 $H_{0}: \ CRIM = -0.1$을 채택한다.


- 이 외에도 신뢰구간에 포함되는 값을 기준으로 검정하면 모두 채택될 것이다.


```python
print(result2.t_test("C(CHAS)[0.0] = C(CHAS)[1.0]"))
```

                                 Test for Constraints                             
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    c0            -2.6867      0.862     -3.118      0.002      -4.380      -0.994
    ==============================================================================
    

- 현재 풀랭크 방식으로 CHAS=0 변수와 CHAS=1 변수를 사용하였다.


- CHAS가 0과 1일 때 차이가 있다는 것을 알 수 있다. 

**전체 회귀계수 검정**

- 모든 회귀계수가 0이 아닌지(유의한지) 확인하는 것으로 이는 결정계수가 0인지에 대한 귀무가설과 동일하다.


- 회귀분석 결과의 F값과 p-value를 통해서도 확인 할 수 있다.

# 4.7 레버리지와 아웃라이어

## 4.7.1 레버리지

실제 종속변수값 $y$가 예측치 $\hat{y}$에 미치는 영향을 나타낸 값


$\hat{y} = X\hat{\beta} = X(X^{T}X)^{-1}X^{T}y = Hy$


$\hat{y}_{i} = \sum_{j}h_{ij}y_{j}$


- 만약 $h_{ii} = 1$ 이고 나머지 성분이 모두 0이면 예측치는 종속변수값과 정확하게 일치한다.


- $0 \leq h_{ii} \leq 1$ 이며 $\sum_{i}h_{ii}=k$ 이다. 일반적으로 모수의 갯수 $k$ 보다 표본의 크기 $n$이 더 크므로 모든 레버리지 값이 1이 될 수는 없다.


- $h_{ii}$의 평균은 $\dfrac{k}{n}$ 이며 일반적으로 이 값보다 2배이상 크면 레버리지가 크다고 한다.

## 4.7.2 statsmodels를 이용한 레버리지 계산


```python
from sklearn.datasets import make_regression

# 샘플 수: 100, 절편: 100, 독립변수 수: 1, 오차의 표준편차: 20
X0, y, coef = make_regression(n_samples=100, n_features=1, noise=20,
                              coef=True, random_state=1)

# 레버리지가 높은 가상의 데이터를 추가
data_100 = (4, 300)
data_101 = (3, 150)

X0 = np.vstack([X0, np.array([data_100[:1], data_101[:1]])])
X = sm.add_constant(X0)
y = np.hstack([y, [data_100[1], data_101[1]]])

plt.scatter(X0, y)

plt.xlabel("x")
plt.ylabel("y")
plt.title("가상의 회귀분석용 데이터")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_130_0.png)
    



```python
# 모델 적합
model = sm.OLS(pd.DataFrame(y), pd.DataFrame(X))
result = model.fit()

# 레버리지 정보
influence = result.get_influence()
hat = influence.hat_matrix_diag

fig, axs = plt.subplots(1,2, figsize=(15, 8))
 
axs[0].stem(hat)
axs[0].axhline(0.02, c="g", ls="--") # 모수의 수: 2 / 표분의수 102 ~ 0.02
axs[0].set_title("각 데이터의 레버리지 값")


axs[1].scatter(X0, y)
sm.graphics.abline_plot(model_results=result, ax=axs[1])

idx = hat > 0.05
axs[1].scatter(X0[idx], y[idx], s=300, c="r", alpha=0.5) # 레버리지가 0.05보다 큰 경우
axs[1].set_title("회귀분석 결과와 레버리지 포인트")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_131_0.png)
    


- 회귀분석 결과에서 `get_influence` 메소드에서 `hat_matrix_diag` 속성으로 레버리지 벡터의 값 얻을 수 있다.


- stem plot으로 레버리지 값을 확인하였고, scatter plot으로 레버리지가 0.05보다 큰 경우 빨간원으로 표시하였다.


```python
print("레버리지의 합:", hat.sum())
print("레버리지의 평균:", hat.mean())
```

    레버리지의 합: 2.0000000000000004
    레버리지의 평균: 0.019607843137254905
    

## 4.7.3 레버리지의 영향


```python
fig, axs = plt.subplots(1,2, figsize=(15, 6))

# 레버리지가 큰 값을 제거 후 적합
model2 = sm.OLS(y[:-2], X[:-2])
result2 = model2.fit()

axs[0].scatter(X0, y)
sm.graphics.abline_plot(model_results=result,
                        c="r", linestyle="--", ax=axs[0])
sm.graphics.abline_plot(model_results=result2,
                        c="g", alpha=0.7, ax=axs[0])

axs[0].plot(X0[-1], y[-1], marker='x', c="r", ms=20, mew=5) # 제거한 값 표시
axs[0].plot(X0[-2], y[-2], marker='x', c="r", ms=20, mew=5) # 제거한 값 표시

axs[0].legend([u"레버리지가 큰 데이터를 포함한 경우", u"레버리지가 큰 데이터를 포함하지 않은 경우"],
           loc="upper left")
axs[0].set_title("레버리지가 높은 데이터가 회귀분석에 미치는 영향")



# 레버리지가 작은 값을 제거 후 적합
model3 = sm.OLS(y[1:], X[1:])
result3 = model3.fit()

axs[1].scatter(X0, y)
sm.graphics.abline_plot(model_results=result,
                        c="r", linestyle="--", ax=axs[1])
sm.graphics.abline_plot(model_results=result3,
                        c="g", alpha=0.7, ax=axs[1])

axs[1].plot(X0[0], y[0], marker='x', c="r", ms=20, mew=5) # 제거한 값 표시

axs[1].legend([u"레버리지가 작은 데이터를 포함한 경우", u"레버리지가 작은 데이터를 포함하지 않은 경우"],
           loc="upper left")
axs[1].set_title("레버리지가 작은 데이터가 회귀분석에 미치는 영향")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_135_0.png)
    


- 레버리지가 높은 데이터를 제거한 경우 모형이 많이 바뀌었으나 작은 데이터를 제거한 경우 큰 차이가 없었다.


- 레버리지가 큰 데이터의 포함유무에 따라 모형이 달라지는 것을 확인 할 수 있다.

## 4.7.4 아웃라이어

모형에서 설명하고 있는 데이터와 동떨어진 값을 가지는 데이터, 즉 잔차가 큰 데이터를 아웃라이어(outlier)라고 한다. 


잔차의 크기는 독립 변수의 영향을 받으므로 아웃라이어를 찾으려면 이 영향을 제거한 표준화 잔차로 확인하여야 한다.

**표준화 잔차**

$e = y - \hat{y} = y- Hy = X\beta + \epsilon - HX\beta - H\epsilon = (I-H)\epsilon$


$Cov(e) = \sigma^{2}(I-H), \quad Var(e_{i}) = \sigma^{2}(1-h_{ii}) \ \approx \ MSE(1-h_{ii})$


$\rightarrow r_{i} = \dfrac{e_{i}}{\sqrt{MSE(1-h_{ii})}}$

## 4.7.5 statsmodels를 이용한 표준화 잔차 계산

**잔차**


```python
plt.figure(figsize=(10, 2))
plt.stem(result.resid)
plt.title("각 데이터의 잔차")
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_143_0.png)
    


**표준화 잔차**


```python
plt.figure(figsize=(10, 2))
plt.stem(result.resid_pearson)
plt.axhline(3, c="g", ls="--")
plt.axhline(-3, c="g", ls="--")
plt.title("각 데이터의 표준화 잔차")
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_145_0.png)
    


- 회귀분석 결과의 `resid_pearson` 속성을 이용해 표준화 잔차를 얻을 수 있다.


- 일반적으로 표준화 잔차가 2~4보다 크면 아웃라이어로 판단한다.

## 4.7.6 Cook's Distance

잔차와 레버리지를 동시에 보기위한 기준으로 Cook’s Distance를 사용하며 다음과 같이 정의된다.

$$ D_i = \frac{e_i^2}{k\text{MSE}}\left[\frac{h_{ii}}{(1-h_{ii})^2}\right] = \frac{r_i^2}{k}\left[\frac{h_{ii}}{(1-h_{ii})}\right]$$

- $k$는 모수의 갯수(상수항 포함)


- 레버리지가 커지거나 잔차의 크기가 커지면 Cook’s Distance 값이 커진다.


- $D_i$에 대한 기준은 다양하지만 $D_i$가 $F(k, n-k)$ 분포를 따르므로 $D_i$가 $F_{0.5}(k, n-k)$ 보다 크면 아웃라이어로 판단한다.


- 혹은 n이 커질수록 $D_i$가 1에 가까워지므로 1보다 크면 아웃라이어로 판단하기도 한다.


- Fox’ Outlier Recommendation은 $D_i$가 $\dfrac{4}{n-k-1}$보다 클 때 아웃라이어로 판단한다.


```python
sm.graphics.plot_leverage_resid2(result)
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_151_0.png)
    


- `plot_leverage_resid2` 를 이용하여 $r_i^2$과 $h_{ii}$를 표시할 수 있다.


- 라벨이 표시된 데이터들이 레버리지가 큰 아웃라이어이다.


- 직접 추가했었던 101, 102번째 값들 역시 확인 된다.


```python
sm.graphics.influence_plot(result, plot_alpha=0.3)
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_153_0.png)
    


- `influence_plot` 를 이용하여 $h_{ii}$와 $r_i$를 표시할 수 있으며 $D_i$를 버블크기로 표시한다.


```python
from statsmodels.graphics import utils

n = len(y) # 샘플 갯수
k = influence.k_vars # 모수 갯수
fox_cr = 4 / (n - k - 1) # criteria 설정

cooks_d2, pvals = influence.cooks_distance

idx = np.where(cooks_d2 > fox_cr)[0] # criteria보다 큰 값들의 위치 (이상값의 위치)

ax = plt.subplot()

plt.scatter(X0, y)
plt.scatter(X0[idx], y[idx], s=300, c="r", alpha=0.5)

# 데이터 라벨
utils.annotate_axes(range(len(idx)), idx,
                    list(zip(X0[idx], y[idx])), [(-20, 15)] * len(idx), size="small", ax=ax)

plt.title("Fox Recommendation으로 선택한 아웃라이어")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_155_0.png)
    


- Fox Recommendation으로 선택한 아웃라이어를 시각화하여 나타냈다.

## 4.7.7 보스턴 집값 예측 예제


```python
from sklearn.datasets import load_boston
boston = load_boston()

# 데이터 불러오기
dfX0 = pd.DataFrame(boston.data, columns=boston.feature_names)
dfX = sm.add_constant(dfX0)
dfy = pd.DataFrame(boston.target, columns=["MEDV"])

# 회귀모형 적합
model_boston = sm.OLS(dfy, dfX)
result_boston = model_boston.fit()
pred = result_boston.predict(dfX) # 예측값

# 영향력 확인
influence_boston = result_boston.get_influence()
cooks_d2, pvals = influence_boston.cooks_distance
n = len(y)
k = influence.k_vars
fox_cr = 4 / (n - k - 1)
idx = np.where(cooks_d2 > fox_cr)[0] 
idx = np.hstack([idx, np.where(boston.target == 50)[0]]) # 이상값 위치에 MEDV가 50인 경우 추가

ax = plt.subplot()

plt.scatter(dfy, pred)
plt.scatter(dfy.iloc[idx], pred[idx], s=300, c="r", alpha=0.5)

utils.annotate_axes(range(len(idx)), idx,
                    list(zip(dfy.MEDV[idx], pred[idx])), [(-20, 15)] * len(idx), size="small", ax=ax)

plt.xlabel("y")
plt.ylabel("Fitted Values")
plt.title("보스턴 집값 데이터에서 아웃라이어")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_158_0.png)
    


# 4.8 분산 분석과 모형 성능

## 4.8.1 분산분석 테이블

| 변동의 요인 | 제곱합 | 제곱합(단순) | 제곱합(행렬) |자유도 | 평균제곱합 | F |
|-|-|-|-|-|-|-|
| 회귀 | ESS |$$\sum(\hat{y}-\bar{y})^{2}$$ | $$Y^{T}(H-\dfrac{1}{n}J)Y$$ | k - 1 | $$MSR = \dfrac{ESS}{k - 1}$$ | $$F^{*} = \dfrac{MSR}{MSE}$$ |
| 오차(잔차) | RSS | $$\sum(y-\hat{y})^{2}$$ | $$Y^{T}(I-H)Y$$ | n - k | $$MSE = \dfrac{RSS}{n - k}$$ | |
| 합계 | TSS | $$\sum(y-\bar{y})^{2}$$| $$Y^{T}(I-\dfrac{1}{n}J)Y$$ | n - 1 |  | |

- $\text{TSS = ESS + RSS}$


- K는 상수항을 포함한 독립변수의 갯수다.


- 평균제곱합의 표기법은 임시로 정해두었다.


```python
print(result_boston.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.741
    Model:                            OLS   Adj. R-squared:                  0.734
    Method:                 Least Squares   F-statistic:                     108.1
    Date:                Fri, 21 May 2021   Prob (F-statistic):          6.72e-135
    Time:                        17:18:01   Log-Likelihood:                -1498.8
    No. Observations:                 506   AIC:                             3026.
    Df Residuals:                     492   BIC:                             3085.
    Df Model:                          13                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const         36.4595      5.103      7.144      0.000      26.432      46.487
    CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043
    ZN             0.0464      0.014      3.382      0.001       0.019       0.073
    INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141
    CHAS           2.6867      0.862      3.118      0.002       0.994       4.380
    NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262
    RM             3.8099      0.418      9.116      0.000       2.989       4.631
    AGE            0.0007      0.013      0.052      0.958      -0.025       0.027
    DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084
    RAD            0.3060      0.066      4.613      0.000       0.176       0.436
    TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005
    PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696
    B              0.0093      0.003      3.467      0.001       0.004       0.015
    LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425
    ==============================================================================
    Omnibus:                      178.041   Durbin-Watson:                   1.078
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
    Skew:                           1.521   Prob(JB):                    8.84e-171
    Kurtosis:                       8.281   Cond. No.                     1.51e+04
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 1.51e+04. This might indicate that there are
    strong multicollinearity or other numerical problems.
    


```python
print("TSS = ", result_boston.centered_tss)
print("ESS = ", result_boston.mse_model*(13))
print("RSS = ", result_boston.ssr)
print("ESS + RSS = ", result_boston.mse_model*(13) + result_boston.ssr)
```

    TSS =  42716.29541501977
    ESS =  31637.510837064794
    RSS =  11078.784577954977
    ESS + RSS =  42716.29541501977
    

## 4.8.2 결정계수

회귀모형에 의해 설명되는 종속변수의 변동 정도

$R^{2} = \dfrac{ESS}{TSS} = 1 - \dfrac{RSS}{TSS}$


```python
print("R squared = ", result_boston.mse_model*(13) / result_boston.centered_tss)
```

    R squared =  0.7406426641094095
    

**$\hat{y}$, $y$의 상관계수 제곱은 결정계수**


```python
temp = pd.concat([pred,dfy], axis=1)
rho = temp.corr().iloc[0,1]

print("rho squared = ", round(rho**2,3))
print("R squared = ", result_boston.rsquared.round(3))
```

    rho squared =  0.741
    R squared =  0.741
    

**조정 결정계수**

결정계수의 단점은 독립변수만 추가하면 실제 독립변수의 유의성여부와 상관없이 RSS가 감소하고 ESS가 증가하여 결정계수가 높아진다.

이를 보완하기위해 조정 결정계수를 사용한다.

$R_{adj}^2 = 1 - \frac{n-1}{n-K}(1-R^2)$


```python
print("adjusted R squared = ", result_boston.rsquared_adj.round(3))
```

    adjusted R squared =  0.734
    


```python
adjR = 1 - (506 - 1) / (506 - 14) * (1 - result_boston.rsquared)
print("adjusted R squared = ", adjR.round(3))
```

    adjusted R squared =  0.734
    

## 4.8.3 절편이 없는 모형

절편이 없는 모형에선 실제 샘플평균과 상관없이 $\bar{y}=0$ 이라는 가정하에 $\text{TSS}$를 계산한다.

이렇게 정의하지 않으면 $\text{TSS = ESS + RSS}$ 관계식이 성립하지 않아서 결정계수의 값이 1보다 커지게 된다.


```python
X0, y, coef = make_regression(
    n_samples=100, n_features=1, noise=30, bias=100, coef=True, random_state=0)
dfX = pd.DataFrame(X0, columns=["X"])
dfy = pd.DataFrame(y, columns=["Y"])
df = pd.concat([dfX, dfy], axis=1)

model2 = sm.OLS.from_formula("Y ~ X + 0", data=df)
result2 = model2.fit()
print(result2.summary())
```

                                     OLS Regression Results                                
    =======================================================================================
    Dep. Variable:                      Y   R-squared (uncentered):                   0.188
    Model:                            OLS   Adj. R-squared (uncentered):              0.179
    Method:                 Least Squares   F-statistic:                              22.87
    Date:                Fri, 21 May 2021   Prob (F-statistic):                    6.03e-06
    Time:                        17:18:01   Log-Likelihood:                         -604.91
    No. Observations:                 100   AIC:                                      1212.
    Df Residuals:                      99   BIC:                                      1214.
    Df Model:                           1                                                  
    Covariance Type:            nonrobust                                                  
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    X             48.8109     10.206      4.783      0.000      28.561      69.061
    ==============================================================================
    Omnibus:                        3.876   Durbin-Watson:                   0.204
    Prob(Omnibus):                  0.144   Jarque-Bera (JB):                2.209
    Skew:                          -0.092   Prob(JB):                        0.331
    Kurtosis:                       2.296   Cond. No.                         1.00
    ==============================================================================
    
    Notes:
    [1] R² is computed without centering (uncentered) since the model does not contain a constant.
    [2] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    


```python
print("기존 평균 사용시 결정계수:", result2.mse_model*(1) / result2.centered_tss)
print("평균 0으로 가정시 결정계수:", result2.mse_model*(1) / result2.uncentered_tss)
print("회귀결과 결정계수:", result2.rsquared)
```

    기존 평균 사용시 결정계수: 0.8336322247273396
    평균 0으로 가정시 결정계수: 0.18768724705943896
    회귀결과 결정계수: 0.18768724705943896
    

- 회귀 결과에서도 결정계수 계산에 대해 절편이 없음을 안내하고 있다.


- 따라서 모형의 결정계수를 비교할 때 절편이 없는 모형과 절편이 있는 모형은 직접 비교하면 안된다.

## 4.8.4 F-검정을 이용한 모형비교


```python
from sklearn.datasets import load_boston

boston = load_boston()
dfX0_boston = pd.DataFrame(boston.data, columns=boston.feature_names)
dfy_boston = pd.DataFrame(boston.target, columns=["MEDV"])
dfX_boston = sm.add_constant(dfX0_boston)
df_boston = pd.concat([dfX_boston, dfy_boston], axis=1)
```


```python
# 전체 모형
model_full = sm.OLS.from_formula(
    "MEDV ~ CRIM + ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS", data=df_boston)

# 축소 모형 (INDUS, AGE 제외)
model_reduced = sm.OLS.from_formula(
    "MEDV ~ CRIM + ZN + NOX + RM + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS", data=df_boston)

# 축소 모형, 전체 모형 순으로 기입
sm.stats.anova_lm(model_reduced.fit(), model_full.fit())
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>df_resid</th>
      <th>ssr</th>
      <th>df_diff</th>
      <th>ss_diff</th>
      <th>F</th>
      <th>Pr(&gt;F)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>494.0</td>
      <td>11081.363952</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>492.0</td>
      <td>11078.784578</td>
      <td>2.0</td>
      <td>2.579374</td>
      <td>0.057274</td>
      <td>0.944342</td>
    </tr>
  </tbody>
</table>
</div>



- 전체 모형과 일부 변수를 제외한 축소 모형을 이용해 F-검정을 실시하여 제외한 일부 변수에 유의성을 확인 할 수 있다.


- 이 경우 유의수준을 0.05로 본다면 귀무가설을 채택해서 INDUS와 AGE는 유의하지 않다고 할 수 있다.

## 4.8.5 F-검정을 사용한 변수 중요도 비교


```python
model_boston = sm.OLS.from_formula(
    "MEDV ~ CRIM + ZN + INDUS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT + CHAS", data=df_boston)
result_boston = model_boston.fit()
sm.stats.anova_lm(result_boston, typ=2)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sum_sq</th>
      <th>df</th>
      <th>F</th>
      <th>PR(&gt;F)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>CRIM</th>
      <td>243.219699</td>
      <td>1.0</td>
      <td>10.801193</td>
      <td>1.086810e-03</td>
    </tr>
    <tr>
      <th>ZN</th>
      <td>257.492979</td>
      <td>1.0</td>
      <td>11.435058</td>
      <td>7.781097e-04</td>
    </tr>
    <tr>
      <th>INDUS</th>
      <td>2.516668</td>
      <td>1.0</td>
      <td>0.111763</td>
      <td>7.382881e-01</td>
    </tr>
    <tr>
      <th>NOX</th>
      <td>487.155674</td>
      <td>1.0</td>
      <td>21.634196</td>
      <td>4.245644e-06</td>
    </tr>
    <tr>
      <th>RM</th>
      <td>1871.324082</td>
      <td>1.0</td>
      <td>83.104012</td>
      <td>1.979441e-18</td>
    </tr>
    <tr>
      <th>AGE</th>
      <td>0.061834</td>
      <td>1.0</td>
      <td>0.002746</td>
      <td>9.582293e-01</td>
    </tr>
    <tr>
      <th>DIS</th>
      <td>1232.412493</td>
      <td>1.0</td>
      <td>54.730457</td>
      <td>6.013491e-13</td>
    </tr>
    <tr>
      <th>RAD</th>
      <td>479.153926</td>
      <td>1.0</td>
      <td>21.278844</td>
      <td>5.070529e-06</td>
    </tr>
    <tr>
      <th>TAX</th>
      <td>242.257440</td>
      <td>1.0</td>
      <td>10.758460</td>
      <td>1.111637e-03</td>
    </tr>
    <tr>
      <th>PTRATIO</th>
      <td>1194.233533</td>
      <td>1.0</td>
      <td>53.034960</td>
      <td>1.308835e-12</td>
    </tr>
    <tr>
      <th>B</th>
      <td>270.634230</td>
      <td>1.0</td>
      <td>12.018651</td>
      <td>5.728592e-04</td>
    </tr>
    <tr>
      <th>LSTAT</th>
      <td>2410.838689</td>
      <td>1.0</td>
      <td>107.063426</td>
      <td>7.776912e-23</td>
    </tr>
    <tr>
      <th>CHAS</th>
      <td>218.970357</td>
      <td>1.0</td>
      <td>9.724299</td>
      <td>1.925030e-03</td>
    </tr>
    <tr>
      <th>Residual</th>
      <td>11078.784578</td>
      <td>492.0</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
</div>



- `anova_lm`의 인자로 `typ=2`로 설정하면 변수를 하나씩 제외해서 F-검정을 한번에 실시한다.


- 이때 p-value는 전체 모형에서 단일 회귀계수의 t-검정의 p-value와 같다.

# 4.9 모형의 진단과 수정

**Boston 회귀결과**


```python
from sklearn.datasets import load_boston

boston = load_boston()

dfX0 = pd.DataFrame(boston.data, columns=boston.feature_names)
dfX = sm.add_constant(dfX0) # 상수항 추가
dfy = pd.DataFrame(boston.target, columns=["MEDV"])
df = pd.concat([dfX, dfy], axis=1)

model_boston = sm.OLS(dfy, dfX)
result_boston = model_boston.fit()
print(result_boston.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.741
    Model:                            OLS   Adj. R-squared:                  0.734
    Method:                 Least Squares   F-statistic:                     108.1
    Date:                Fri, 21 May 2021   Prob (F-statistic):          6.72e-135
    Time:                        17:18:01   Log-Likelihood:                -1498.8
    No. Observations:                 506   AIC:                             3026.
    Df Residuals:                     492   BIC:                             3085.
    Df Model:                          13                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    const         36.4595      5.103      7.144      0.000      26.432      46.487
    CRIM          -0.1080      0.033     -3.287      0.001      -0.173      -0.043
    ZN             0.0464      0.014      3.382      0.001       0.019       0.073
    INDUS          0.0206      0.061      0.334      0.738      -0.100       0.141
    CHAS           2.6867      0.862      3.118      0.002       0.994       4.380
    NOX          -17.7666      3.820     -4.651      0.000     -25.272     -10.262
    RM             3.8099      0.418      9.116      0.000       2.989       4.631
    AGE            0.0007      0.013      0.052      0.958      -0.025       0.027
    DIS           -1.4756      0.199     -7.398      0.000      -1.867      -1.084
    RAD            0.3060      0.066      4.613      0.000       0.176       0.436
    TAX           -0.0123      0.004     -3.280      0.001      -0.020      -0.005
    PTRATIO       -0.9527      0.131     -7.283      0.000      -1.210      -0.696
    B              0.0093      0.003      3.467      0.001       0.004       0.015
    LSTAT         -0.5248      0.051    -10.347      0.000      -0.624      -0.425
    ==============================================================================
    Omnibus:                      178.041   Durbin-Watson:                   1.078
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
    Skew:                           1.521   Prob(JB):                    8.84e-171
    Kurtosis:                       8.281   Cond. No.                     1.51e+04
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 1.51e+04. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

## 4.9.1 오차의 정규성


```python
figure, axs = plt.subplots(1,3, figsize = (15,6))

# 잔차 Q-Q  plot
# sm.ProbPlot(fit1.resid).qqplot(line="s",ax=axs[0])
sp.stats.probplot(result_boston.resid, dist='norm', fit=True, plot=axs[0])

# 잔차의 히스토그램
sns.distplot(x = result_boston.resid , ax = axs[1])
axs[1].set_xlabel("Residuals")
axs[1].set_title("Residuals Histogram")

# 잔차도
yhat = result_boston.fittedvalues
temp = pd.concat( [ yhat, dfy ], axis = 1)
temp.columns = [ "yhat", "MEDV" ]

sns.residplot(x = "yhat", y = "MEDV", 
              data = temp, 
              scatter_kws = {'edgecolor':"white", "alpha":0.7},
              line_kws = {"color":"red"},
              ax = axs[2])

MSE_sq = np.sqrt(result_boston.mse_resid)


for i, c in enumerate(["red", "green", "black"]):
    axs[2].axhline( (i+1) * MSE_sq, color = c)
    axs[2].axhline( -(i+1) * MSE_sq, color = c)
    
    axs[2].text(47, (i+1) * MSE_sq, f"{i+1}"r'${}\sqrt{MSE}$')
    axs[2].text(47, -(i+1) * MSE_sq, f"-{i+1}"r'${}\sqrt{MSE}$')

axs[2].set_xlabel("Fitted Values")
axs[2].set_ylabel("Standardized Residuals")
axs[2].set_title("Residuals Diagram")


plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_192_0.png)
    


- 잔차 Q-Q Plot을 보았을때 $45^{\circ}$를 벗어나는 케이스가 많다.


- 잔차 히스토그램에선 잔차가 큰 값들이 다수 보여 오른쪽으로 꼬리가 생겨 정규분포로 보기 어렵다.


```python
r1 = result_boston.resid[ (result_boston.resid > -MSE_sq) & (result_boston.resid < MSE_sq)].count() / result_boston.resid.count() * 100
r2 = result_boston.resid[ (result_boston.resid > -2*MSE_sq) & (result_boston.resid < 2*MSE_sq)].count() / result_boston.resid.count() * 100
r3 = result_boston.resid[ (result_boston.resid > -3*MSE_sq) & (result_boston.resid < 3*MSE_sq)].count() / result_boston.resid.count() * 100

print(f"루트 1MSE 구간에 포함된 잔차 비율: {r1:1.2f}%")
print(f"루트 2MSE 구간에 포함된 잔차 비율: {r2:1.2f}%")
print(f"루트 3MSE 구간에 포함된 잔차 비율: {r3:1.2f}%")
```

    루트 1MSE 구간에 포함된 잔차 비율: 78.06%
    루트 2MSE 구간에 포함된 잔차 비율: 95.26%
    루트 3MSE 구간에 포함된 잔차 비율: 98.62%
    

- 잔차가 정규분포를 따른다면 대략적으로 

    $\pm\sqrt{MSE}$ 안에 전체 잔차의 68%,
    
    $\pm2\sqrt{MSE}$ 안에 전체 잔차의 95%,
    
    $\pm3\sqrt{MSE}$ 안에 전체 잔차의 99%가 포함된다.

    대부분 만족을 하지만 몇 개의 아웃라이어가 확인된다.


```python
omni_result = sm.stats.omni_normtest(result_boston.resid)

print('statistic: {0:.3f}'.format(omni_result.statistic))
print('p-value: {0:.3f}'.format(omni_result.pvalue))
```

    statistic: 178.041
    p-value: 0.000
    

- 정규성 검정에서도 잔차가 정규분포를 따르지 않는다.


- 이는 회귀분석 결과에서도 확인 가능하다.

## 4.9.2 오차의 등분산성

선형 회귀 모형에서는 종속 변수 값의 분산이 독립 변수의 값과 상관없이 고정된 값을 가져야한다. 

그러나 실제 데이터는 독립 변수 값의 크기가 커지면 종속 변수 값의 분산도 커지는 이분산성 문제가 발생한다. 

종속변수를 로그 변환한 트랜스로그 모형을 사용하면 이분산성 문제가 해결되는 경우도 있다.


```python
fig = plt.figure(figsize= (10,8))

p = sns.residplot(x = "yhat", y = "MEDV", 
                  data = temp, 
                  scatter_kws = {'edgecolor':"white", "alpha":0.7},
                  lowess = True,
                  line_kws = {"color":"red"})

p.set_title("Residuals Diagram", fontsize=15, y=1.02)
p.set(xlabel= "Fitted Values", ylabel = "Residuals")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_200_0.png)
    


- 등분산성이 만족된다면 Fitted Values(혹은 독립변수)의 값과 상관없이 같은 퍼짐 정도를 가져야한다. 

    즉, Fitted Values값에 상관없이 랜덤하게 분포되어 있어야 한다.
    

- 현재 잔차도로 확인하였을 때는 랜덤하게 분포되어 있는 것으로 보이며 등분산성을 만족하는 것으로 판단된다.

## 4.9.3 오차의 독립성

Durbin-Watson 검정통계량

- 0에 가까울수록 잔차들은 양의 자기상관을 가지며, 


- 2에 가까울수록 자기상관이 없어지며, 


- 2보다 크고 4에 가까워질수록 잔차들은 음의 자기상관을 갖는다. 


- 일반적으로 1.5 ~ 2.5사이 값이면 자기상관관계가 없어 독립이라고 판단한다.

자기상관관계가 있으면 MSE는 오차분산을 과소 추정하게 되어 t값, F값, 결정계수 값이 크게 나와 귀무가설을 기각하게 되어

실제로 유의미하지 않은 결과를 유의미한 결과로 왜곡하게 된다.


```python
print(f"D-W 통계량: {round(durbin_watson(result_boston.resid),3)}")
```

    D-W 통계량: 1.078
    

- D-W 통계량이 0에 가까워 양의 자기상관관계가 있다고 판단된다.


- 이는 회귀분석 결과에서도 확인 가능하다.

## 4.9.4 비선형 변형

독립변수와 종속변수간의 관계가 비선형이라면 독립변수 혹은 종속변수를 비선형 변환 할 수 있다.

**독립변수 변환**


```python
from sklearn.datasets import load_boston

boston = load_boston()
dfX = pd.DataFrame(boston.data, columns=boston.feature_names)
dfy = pd.DataFrame(boston.target, columns=["MEDV"])
df_boston = pd.concat([dfX, dfy], axis=1)

sns.scatterplot(x="LSTAT", y="MEDV", data=df_boston)
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_209_0.png)
    


- LSTAT과 MEDV의 산점도를 보았을 때 비선형관계를 확인 할 수 있다.


```python
model2 = sm.OLS.from_formula("MEDV ~ LSTAT + I(LSTAT**2)", data=df_boston)
result2 = model2.fit()
print(result2.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.641
    Model:                            OLS   Adj. R-squared:                  0.639
    Method:                 Least Squares   F-statistic:                     448.5
    Date:                Fri, 21 May 2021   Prob (F-statistic):          1.56e-112
    Time:                        17:18:03   Log-Likelihood:                -1581.3
    No. Observations:                 506   AIC:                             3169.
    Df Residuals:                     503   BIC:                             3181.
    Df Model:                           2                                         
    Covariance Type:            nonrobust                                         
    =================================================================================
                        coef    std err          t      P>|t|      [0.025      0.975]
    ---------------------------------------------------------------------------------
    Intercept        42.8620      0.872     49.149      0.000      41.149      44.575
    LSTAT            -2.3328      0.124    -18.843      0.000      -2.576      -2.090
    I(LSTAT ** 2)     0.0435      0.004     11.628      0.000       0.036       0.051
    ==============================================================================
    Omnibus:                      107.006   Durbin-Watson:                   0.921
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              228.388
    Skew:                           1.128   Prob(JB):                     2.55e-50
    Kurtosis:                       5.397   Cond. No.                     1.13e+03
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 1.13e+03. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

- `I()` 연산자를 이용해 독립변수 LSTAT의 다항식을 포함하였다.


```python
y_hat2 = result2.predict(df_boston)
df2 = pd.concat([y_hat2, df_boston.LSTAT], axis=1).sort_values("LSTAT")
df2.columns = ["Prediction", "LSTAT"]

fig, axs = plt.subplots(1,2, figsize=(15,8))

# 독립변수와 종속변수 산점도 및 회귀식
axs[0].plot(df_boston.LSTAT, df_boston.MEDV, "bo", alpha=0.5)
df2.plot(x="LSTAT", y="Prediction", style="r-", lw=3, ax=axs[0])
axs[0].set_title("MEDV ~ LSTAT + I(LSTAT**2)")

# 종속변수와 예측치 산점도
axs[1].scatter(boston.target, y_hat2)
axs[1].set_title("Fitted Values vs dependet variable")

plt.tight_layout()
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_213_0.png)
    


- 산점도를 보면 예측치가 어느정도 데이터의 분포에 맞게 잘 적합된 것으로 보인다.


- 종속변수와 예측치의 산점를 보면 어느정도 선형적인 모습이 나타난다.

**종속변수 변환**


```python
model3 = sm.OLS.from_formula("np.log(MEDV) ~ LSTAT", data=df_boston)
result3 = model3.fit()
print(result3.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:           np.log(MEDV)   R-squared:                       0.648
    Model:                            OLS   Adj. R-squared:                  0.647
    Method:                 Least Squares   F-statistic:                     928.1
    Date:                Fri, 21 May 2021   Prob (F-statistic):          2.23e-116
    Time:                        17:18:03   Log-Likelihood:               -0.57634
    No. Observations:                 506   AIC:                             5.153
    Df Residuals:                     504   BIC:                             13.61
    Df Model:                           1                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept      3.6176      0.022    164.654      0.000       3.574       3.661
    LSTAT         -0.0461      0.002    -30.465      0.000      -0.049      -0.043
    ==============================================================================
    Omnibus:                       27.562   Durbin-Watson:                   0.909
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):               50.719
    Skew:                           0.351   Prob(JB):                     9.69e-12
    Kurtosis:                       4.383   Cond. No.                         29.7
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    

- MEDV를 로그 변환하였다.


```python
y_hat3 = result3.predict(df_boston)
df2 = pd.concat([y_hat3, df_boston.LSTAT], axis=1).sort_values("LSTAT")
df2.columns = ["Prediction", "LSTAT"]

fig, axs = plt.subplots(1,2, figsize=(15,8))

# 독립변수와 종속변수 산점도 및 회귀식
axs[0].plot(df_boston.LSTAT, np.log(df_boston.MEDV), "bo", alpha=0.5)
df2.plot(x="LSTAT", y="Prediction", style="r-", lw=3, ax=axs[0])
axs[0].set_title("np.log(MEDV) ~ LSTAT")

# 종속변수와 예측치 산점도
axs[1].scatter(boston.target, y_hat3)
axs[1].set_title("Fitted Values vs dependet variable")

plt.tight_layout()
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_218_0.png)
    


- 산점도를 보면 예측치가 어느정도 데이터의 분포에 맞게 잘 적합된 것으로 보이며 종속변수의 변화로 인해 선형관계가 보인다.


- 종속변수와 예측치의 산점를 보면 비선형 관계도 보이지만 어느정도 선형적인 모습이 나타난다.

# 5.1 과최적화(Overfitting)

모형을 특정 샘플 데이터에 대해 과도하게 최적화하는 것을 과최적화(overfitting)라고 한다.

과최적화 발생조건:

- 독립 변수 데이터 갯수에 비해 모형 모수의 수가 과도하게 크거나


- 독립 변수 데이터가 서로 독립이 아닌 경우에 발생한다.


과최적화 문제점:

- 트레이닝 데이터에 사용되지 않은 새로운 독립 변수 값을 입력하면 오차가 커진다. (cross-validation 오차)


- 샘플이 조금만 변화해도 회귀계수의 값이 크게 달라진다. (추정의 부정확함)

**비선형 데이터 생성**


```python
def make_nonlinear(seed=0):
    np.random.seed(seed)
    n_samples = 30
    X = np.sort(np.random.rand(n_samples)) # uniform random sample
    y = np.sin(2 * np.pi * X) + np.random.randn(n_samples) * 0.1 # 2*sin(pi)*x + z random sample*0.1
    X = X[:, np.newaxis] # 2차원 배열
    return (X, y)

X, y = make_nonlinear()
plt.scatter(X, y)
plt.xlabel("x")
plt.ylabel("y")
plt.title("비선형 데이터의 예")
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_223_0.png)
    


**다항회귀모형**


```python
def polyreg(degree, seed=0, ax=None):
    X, y = make_nonlinear(seed)

    dfX = pd.DataFrame(X, columns=["x"])
    dfX = sm.add_constant(dfX)
    dfy = pd.DataFrame(y, columns=["y"])
    df = pd.concat([dfX, dfy], axis=1)

    model_str = "y ~ "
    for i in range(degree):
        if i == 0:
            prefix = ""
        else:
            prefix = " + "
        model_str += prefix + "I(x**{})".format(i + 1)
    model = sm.OLS.from_formula(model_str, data=df)
    result = model.fit()

    if ax:
        ax.scatter(X, y)
        xx = np.linspace(0, 1, 1000)
        dfX_new = pd.DataFrame(xx[:, np.newaxis], columns=["x"])
        ax.plot(xx, result.predict(dfX_new), "r-")
        ax.set_ylim(-3, 3)
        ax.set_title("차수={}, 시드값={}".format(degree, seed))
        xlabel = "\n".join(str(result.params).split("\n")[:-1])
        font = {'family': 'NanumGothicCoding', 'color':  'black', 'size': 10}
        ax.set_xlabel(xlabel, fontdict=font)

    return result
```


```python
ax1 = plt.subplot(131)
polyreg(1, ax=ax1)
ax2 = plt.subplot(132)
polyreg(2, ax=ax2)
ax3 = plt.subplot(133)
polyreg(3, ax=ax3)
plt.tight_layout()
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_226_0.png)
    


- 실제 자료가 비선형 모형일 때 다항회귀모형이 더 잘 적합한 모형임을 알 수 있다.

**모수를 늘린 경우**


```python
ax1 = plt.subplot(131)
polyreg(3, ax=ax1)
ax2 = plt.subplot(132)
polyreg(20, ax=ax2)
ax3 = plt.subplot(133)
polyreg(30, ax=ax3)
plt.tight_layout()
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_229_0.png)
    


- 모수의 수(다항식의 수 = 새로운 독립변수)를 늘림에 따라 오차가 커짐을 알 수 있다.

**샘플이 바뀌는 경우**


```python
ax1 = plt.subplot(131)
polyreg(10, seed=1, ax=ax1)
ax2 = plt.subplot(132)
polyreg(10, seed=2,  ax=ax2)
ax3 = plt.subplot(133)
polyreg(10, seed=3,  ax=ax3)
plt.tight_layout()
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_232_0.png)
    


- 같은 모형이지만 샘플이 바뀔 때 마다 회귀계수가 크게 달라짐을 확인 할 수 있다.

# 5.2 교차검증

## 5.2.1 statsmodels 패키지의 교차검증

statsmodels 패키지에는 교차검증을 위한 기능이 별도로 준비되어 있지 않고 사용자가 직접 코드를 작성해야 한다.


```python
from sklearn.datasets import load_boston

boston = load_boston()
dfX = pd.DataFrame(boston.data, columns=boston.feature_names)
dfy = pd.DataFrame(boston.target, columns=["MEDV"])
df = pd.concat([dfX, dfy], axis=1)

N = len(df) # boston row
ratio = 0.7

np.random.seed(0)
idx_train = np.random.choice(np.arange(N), np.int(ratio * N)) # 원 데이터의 70% train set
idx_test = list(set(np.arange(N)).difference(idx_train)) # 30% test set

df_train = df.iloc[idx_train]
df_test = df.iloc[idx_test]
```


```python
model = sm.OLS.from_formula("MEDV ~ " + "+".join(boston.feature_names), data=df_train)
result = model.fit()
print(f"train set 결정계수: {result.rsquared:0.3f}")
```

    train set 결정계수: 0.757
    


```python
pred = result.predict(df_test)

rss = ((df_test.MEDV - pred) ** 2).sum()
tss = ((df_test.MEDV - df_test.MEDV.mean())** 2).sum()
rsquared = 1 - rss / tss
print(f"test set 결정계수: {rsquared:0.3f}")
```

    test set 결정계수: 0.688
    

## 5.2.2 scikit-learn 패키지의 교차검증

**데이터 분리**

`sklearn.model_selection`의 `train_test_split` 명령어로 데이터 분리가 가능하다.


`train_test_split(data, data2, test_size, train_size, random_state)`


- data: 독립 변수 데이터 배열 또는 pandas 데이터프레임


- data2: 종속 변수 데이터. data 인수에 종속 변수 데이터가 같이 있으면 생략할 수 있다.


- test_size: 검증용 데이터 개수. 1보다 작은 실수이면 비율을 나타낸다.


- train_size: 학습용 데이터의 개수. 1보다 작은 실수이면 비율을 나타낸다. test_size와 train_size 중 하나만 있어도 된다.


- random_state: 난수 시드


```python
# 데이터 프레임에 독립변수, 종속변수가 같이 있는 경우
from sklearn.model_selection import train_test_split

df_train, df_test = train_test_split(df, test_size=0.3, random_state=0)
df_train.shape, df_test.shape
```




    ((354, 14), (152, 14))




```python
# 독립변수, 종속변수 데이터가 따로 있는 경우
dfX_train, dfX_test, dfy_train, dfy_test = train_test_split(dfX, dfy, test_size=0.3, random_state=0)
dfX_train.shape, dfy_train.shape, dfX_test.shape, dfy_test.shape
```




    ((354, 13), (354, 1), (152, 13), (152, 1))



## 5.2.3 K-폴드 교차검증

`sklearn.model_selection`의 `KFold`를 이용해 K-폴드 교차검증을 진행할 수 있다.

`sklearn.metrics`에는 예측성능을 평가하기 위한 다양한 함수가 내제되어있다.


```python
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score

scores = np.zeros(5)
cv = KFold(5, shuffle=True, random_state=0) # k=5

# cv.split으로 train, test 인덱스 반환
for i, (idx_train, idx_test) in enumerate(cv.split(df)):
    df_train = df.iloc[idx_train]
    df_test = df.iloc[idx_test]
    
    model = sm.OLS.from_formula("MEDV ~ " + "+".join(boston.feature_names), data=df_train)
    result = model.fit()
    
    pred = result.predict(df_test)
    rsquared = r2_score(df_test.MEDV, pred)
    
    scores[i] = rsquared
    print("학습 R2 = {:.8f}, 검증 R2 = {:.8f}".format(result.rsquared, rsquared))
```

    학습 R2 = 0.77301356, 검증 R2 = 0.58922238
    학습 R2 = 0.72917058, 검증 R2 = 0.77799144
    학습 R2 = 0.74897081, 검증 R2 = 0.66791979
    학습 R2 = 0.75658611, 검증 R2 = 0.66801630
    학습 R2 = 0.70497483, 검증 R2 = 0.83953317
    

## 5.2.4 교차검증 반복

`sklearn.model_selection`의 `cross_val_score`를 이용해 교차검증 반복을 쉽게 진행할 수 있다.

`cross_val_score`는 `sklearn`의 모형만 사용가능하다.

`cross_val_score(model, X, y, scoring=None, cv=None)`

- model: 회귀 분석 모형


- X: 독립 변수 데이터


- y: 종속 변수 데이터


- scoring: 성능 검증에 사용할 함수 이름


- cv: 교차검증 생성기 객체 또는 숫자로 None이면 KFold(3) 숫자 k이면 KFold(k)

**statsmodels 모형을 사용할 경우**


```python
from sklearn.base import BaseEstimator, RegressorMixin
import statsmodels.formula.api as smf
import statsmodels.api as sm

class StatsmodelsOLS(BaseEstimator, RegressorMixin):
    def __init__(self, formula):
        self.formula = formula
        self.model = None
        self.data = None
        self.result = None
        
    def fit(self, dfX, dfy):
        self.data = pd.concat([dfX, dfy], axis=1)
        self.model = smf.ols(self.formula, data=self.data)
        self.result = self.model.fit()
        
    def predict(self, new_data):
        return self.result.predict(new_data)
```


```python
from sklearn.model_selection import cross_val_score

model = StatsmodelsOLS("MEDV ~ " + "+".join(boston.feature_names))
cv = KFold(5, shuffle=True, random_state=0)

cross_val_score(model, dfX, dfy, scoring="r2", cv=cv) # 결정계수 출력
```




    array([0.58922238, 0.77799144, 0.66791979, 0.6680163 , 0.83953317])



**sklearn 모형을 사용할 경우**


```python
from sklearn.linear_model import LinearRegression

model = LinearRegression(fit_intercept=True).fit(dfX,dfy)

cross_val_score(model, dfX, dfy, scoring="r2", cv=cv) # 결정계수 출력
```




    array([0.58922238, 0.77799144, 0.66791979, 0.6680163 , 0.83953317])



# 5.3 다중공선성과 변수선택

다중공선성: 독립 변수의 일부가 다른 독립 변수의 조합으로 표현될 수 있는 경우로서 즉, 선형 독립이 아닌 경우이다.

이는 독립 변수의 공분산 행렬이 full rank 이어야 한다는 조건을 침해한다.

**예제 데이터**


```python
from statsmodels.datasets.longley import load_pandas

dfy = load_pandas().endog # 종속변수 이름: TOTEMP
dfX = load_pandas().exog
df = pd.concat([dfy, dfX], axis=1)

corr_M = dfX.corr() # 독립변수 상관관계
mask = np.array(corr_M)
mask[np.tril_indices_from(mask)] = False

plt.figure(figsize= (10,8))

sns.heatmap(corr_M, 
            cmap = sns.light_palette("red", as_cmap=True),
            annot = True, 
            mask = mask)

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_259_0.png)
    


- 독립변수간의 상관관계가 강한 경우가 많다.

## 5.3.1 다중공선성 문제점: 조건수 증가


```python
from sklearn.model_selection import train_test_split

def get_model1(seed):
    df_train, df_test = train_test_split(df, test_size=0.5, random_state=seed)
    model = sm.OLS.from_formula("TOTEMP ~ GNPDEFL + POP + GNP + YEAR + ARMED + UNEMP", data=df_train)
    return df_train, df_test, model.fit()


df_train, df_test, result1 = get_model1(3)
print(result1.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                 TOTEMP   R-squared:                       1.000
    Model:                            OLS   Adj. R-squared:                  0.997
    Method:                 Least Squares   F-statistic:                     437.5
    Date:                Fri, 21 May 2021   Prob (F-statistic):             0.0366
    Time:                        17:18:07   Log-Likelihood:                -44.199
    No. Observations:                   8   AIC:                             102.4
    Df Residuals:                       1   BIC:                             103.0
    Df Model:                           6                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept  -1.235e+07   2.97e+06     -4.165      0.150      -5e+07    2.53e+07
    GNPDEFL      106.2620     75.709      1.404      0.394    -855.708    1068.232
    POP            2.2959      0.725      3.167      0.195      -6.915      11.506
    GNP           -0.3997      0.120     -3.339      0.185      -1.920       1.121
    YEAR        6300.6231   1498.900      4.203      0.149   -1.27e+04    2.53e+04
    ARMED         -0.2450      0.402     -0.609      0.652      -5.354       4.864
    UNEMP         -6.3311      1.324     -4.782      0.131     -23.153      10.491
    ==============================================================================
    Omnibus:                        0.258   Durbin-Watson:                   1.713
    Prob(Omnibus):                  0.879   Jarque-Bera (JB):                0.304
    Skew:                           0.300   Prob(JB):                        0.859
    Kurtosis:                       2.258   Cond. No.                     2.01e+10
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 2.01e+10. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

- 다중공선성이 존재시 조건수도 많아지게 된다.

## 5.3.2 다중공선성 문제점: 과최적화


```python
from sklearn.metrics import r2_score

test1 = []

for i in range(10):
    df_train, df_test, result = get_model1(i)
    
    pred_test = result.predict(df_test)
    rsquared = r2_score(df_test.TOTEMP, pred_test)
    test1.append(round(rsquared,3))

test1
```




    [0.982, 0.974, 0.988, 0.759, 0.981, 0.894, 0.88, 0.931, 0.861, 0.968]



- train에선 결정계수가 1이 었으나 test에선 모두 train보다 결정계수가 작게 나왔다.

## 5.3.3 다중공선성 해결법

- 변수 선택법으로 의존적인 변수 삭제


- PCA(principal component analysis) 방법으로 의존적인 성분 삭제


- 정규화(regularized) 방법 사용

## 5.3.4 분산팽창계수(VIF)

다중 공선성을 없애는 가장 기본적인 방법은 다른 독립변수에 의존하는 변수를 없애는 것이다. 

가장 의존적인 독립변수를 선택하는 방법으로는 VIF(Variance Inflation Factor)를 사용할 수 있다. 

- VIF는 특정 독립변수를 나머지 독립변수로 적합했을 때 성능을 나타낸 것이다.


- 다른 변수에 의존할수록 VIF 값이 높다.

`statsmodels.stats.outliers_influence`의 `variance_inflation_factor`를 이용하여 VIF를 계산할 수 있다.


```python
from statsmodels.stats.outliers_influence import variance_inflation_factor

vif = pd.DataFrame()

lst = []
for i in range(dfX.shape[1]):
    v = variance_inflation_factor(dfX.values, i) 
    lst.append(v)
    
vif["VIF Factor"] = lst
vif["features"] = dfX.columns
vif
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>VIF Factor</th>
      <th>features</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>12425.514335</td>
      <td>GNPDEFL</td>
    </tr>
    <tr>
      <th>1</th>
      <td>10290.435437</td>
      <td>GNP</td>
    </tr>
    <tr>
      <th>2</th>
      <td>136.224354</td>
      <td>UNEMP</td>
    </tr>
    <tr>
      <th>3</th>
      <td>39.983386</td>
      <td>ARMED</td>
    </tr>
    <tr>
      <th>4</th>
      <td>101193.161993</td>
      <td>POP</td>
    </tr>
    <tr>
      <th>5</th>
      <td>84709.950443</td>
      <td>YEAR</td>
    </tr>
  </tbody>
</table>
</div>




```python
def get_model2(seed):
    df_train, df_test = train_test_split(df, test_size=0.5, random_state=seed)
    model = sm.OLS.from_formula("TOTEMP ~ GNP + ARMED + UNEMP", data=df_train)
    return df_train, df_test, model.fit()


df_train, df_test, result2 = get_model2(3)
print(result2.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                 TOTEMP   R-squared:                       0.989
    Model:                            OLS   Adj. R-squared:                  0.981
    Method:                 Least Squares   F-statistic:                     118.6
    Date:                Fri, 21 May 2021   Prob (F-statistic):           0.000231
    Time:                        17:18:07   Log-Likelihood:                -57.695
    No. Observations:                   8   AIC:                             123.4
    Df Residuals:                       4   BIC:                             123.7
    Df Model:                           3                                         
    Covariance Type:            nonrobust                                         
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    Intercept   5.399e+04   1013.271     53.281      0.000    5.12e+04    5.68e+04
    GNP            0.0500      0.005     10.669      0.000       0.037       0.063
    ARMED         -1.2804      0.497     -2.574      0.062      -2.662       0.101
    UNEMP         -1.4265      0.363     -3.931      0.017      -2.434      -0.419
    ==============================================================================
    Omnibus:                        0.628   Durbin-Watson:                   2.032
    Prob(Omnibus):                  0.731   Jarque-Bera (JB):                0.565
    Skew:                           0.390   Prob(JB):                        0.754
    Kurtosis:                       1.958   Cond. No.                     2.44e+06
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 2.44e+06. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

- VIF계수가 작은 GNP, ARMED, UNEMP만을 사용하였을 때 여전히 조건수가 많지만 이전보다 줄어든 것을 알 수 있다.


```python
df_train.std()
```




    TOTEMP      3325.272051
    GNPDEFL       10.975224
    GNP        92714.595875
    UNEMP       1028.961751
    ARMED        678.062523
    POP         6499.246792
    YEAR           4.566962
    dtype: float64



- 변수별 단위차가 있어 스케일링이 필요하다.


```python
def get_model3(seed):
    df_train, df_test = train_test_split(df, test_size=0.5, random_state=seed)
    model = sm.OLS.from_formula("TOTEMP ~ scale(GNP) + scale(ARMED) + scale(UNEMP)", data=df_train)
    return df_train, df_test, model.fit()


df_train, df_test, result3 = get_model3(3)
print(result3.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                 TOTEMP   R-squared:                       0.989
    Model:                            OLS   Adj. R-squared:                  0.981
    Method:                 Least Squares   F-statistic:                     118.6
    Date:                Fri, 21 May 2021   Prob (F-statistic):           0.000231
    Time:                        17:18:07   Log-Likelihood:                -57.695
    No. Observations:                   8   AIC:                             123.4
    Df Residuals:                       4   BIC:                             123.7
    Df Model:                           3                                         
    Covariance Type:            nonrobust                                         
    ================================================================================
                       coef    std err          t      P>|t|      [0.025      0.975]
    --------------------------------------------------------------------------------
    Intercept     6.538e+04    163.988    398.686      0.000    6.49e+04    6.58e+04
    scale(GNP)    4338.7051    406.683     10.669      0.000    3209.571    5467.839
    scale(ARMED)  -812.1407    315.538     -2.574      0.062   -1688.215      63.933
    scale(UNEMP) -1373.0426    349.316     -3.931      0.017   -2342.898    -403.187
    ==============================================================================
    Omnibus:                        0.628   Durbin-Watson:                   2.032
    Prob(Omnibus):                  0.731   Jarque-Bera (JB):                0.565
    Skew:                           0.390   Prob(JB):                        0.754
    Kurtosis:                       1.958   Cond. No.                         4.77
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    

- 스케일링으로 인해 조건수에 대한 경고가 더 이상 나타나지 않는다.


```python
from sklearn.metrics import r2_score

test2 = []

for i in range(10):
    df_train, df_test, result = get_model3(i)
    
    pred_test = result.predict(df_test)
    rsquared = r2_score(df_test.TOTEMP, pred_test)
    test2.append(round(rsquared,3))

test2
```




    [0.976, 0.984, 0.969, 0.94, 0.977, 0.956, 0.98, 0.992, 0.984, 0.979]



- train에서의 결정계수 0.989와 test 결정계수가 어느정도 비슷하게 나타나 이전보다 차이가 줄었다.


```python
plt.subplot(121)
plt.plot(test1, 'ro', label="검증 성능")
plt.axhline(result1.rsquared, label="학습 성능")
plt.legend()
plt.xlabel("시드값")
plt.ylabel("성능(결정계수)")
plt.title("다중공선성 제거 전")
plt.ylim(0.7, 1.1)

plt.subplot(122)
plt.plot(test2, 'ro', label="검증 성능")
plt.axhline(result2.rsquared, label="학습 성능")
plt.legend()
plt.xlabel("시드값")
plt.ylabel("성능(결정계수)")
plt.title("다중공선성 제거 후")
plt.ylim(0.7, 1.1)

plt.suptitle("다중공선성 제거 전과 제거 후의 성능 비교", y=1.04)
plt.tight_layout()
plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_281_0.png)
    


# 5.4 정규화 선형회귀

정규화 선형회귀 방법은 선형회귀 계수에 대한 제약 조건(패널티)을 추가함으로써 과최적화를 막는 방법이다.

모형이 과도하게 최적화되면 모형 계수의 크기도 과도하게 증가하는 경향이 나타난다. 

따라서 정규화 방법에서 추가하는 제약 조건은 일반적으로 계수의 크기를 제한하는 방법이다.

## 5.4.1 Ridge 회귀모형

Ridge 회귀모형에서는 가중치(회귀계수)들의 제곱합을 최소화하는 것을 추가적인 제약 조건으로 한다.

$$
\beta = \text{arg}\min_\beta \left( \sum_{i=1}^n e_i^2 + \lambda \sum_{j=1}^k \beta_j^2 \right)
$$

$\lambda$는 기존의 잔차 제곱합과 추가적 제약 조건의 비중을 조절하기 위한 하이퍼 모수(hyper parameter)이다. $\lambda$가 크면 정규화 정도가 커지고 가중치의 값들이 작아진다. $\lambda$가 작아지면 정규화 정도가 작아지며 $\lambda$ 가 0이 되면 일반적인 선형 회귀모형이 된다.

## 5.4.2 Lasso 회귀모형

Lasso 회귀모형은 가중치(회귀계수)의 절대값의 합을 최소화하는 것을 추가적인 제약 조건으로 한다.

$$
\beta = \text{arg}\min_\beta \left( \sum_{i=1}^n e_i^2 + \lambda \sum_{j=1}^k | \beta_j | \right)
$$


## 5.4.3 Elastic Net 회귀모형

Elastic Net 회귀모형은 가중치(회귀계수)의 절대값의 합과 제곱합을 동시에 제약 조건으로 가지는 모형이다.

$$
\beta = \text{arg}\min_\beta \left( \sum_{i=1}^n e_i^2 + \lambda_1 \sum_{j=1}^k | \beta_j | + \lambda_2 \sum_{j=1}^k \beta_j^2 \right)
$$

$\lambda_1$, $\lambda_2$ 두 개의 하이퍼 모수를 가진다.

## 5.4.4 statsmodels의 정규화 회귀모형

회귀 모형의 `fit_regularized` 메소드를 이용하여  Elastic Net 모형 계수를 구할 수 있다.

$$
0.5 \times \text{RSS}/n + \text{alpha} \times \big( 0.5 \times (1-\text{L1_wt})\sum \beta_i^2 + \text{L1_wt} \sum |\beta_i| \big)
$$

- 하이퍼모수는 alpha와 L1_wt이다.

**비선형 데이터 생성**


```python
def make_nonlinear(seed=0):
    np.random.seed(seed)
    n_samples = 30
    X = np.sort(np.random.rand(n_samples))
    y = np.sin(2 * np.pi * X) + np.random.randn(n_samples) * 0.1
    X = X[:, np.newaxis]
    return (X, y)


X, y = make_nonlinear()

dfX = pd.DataFrame(X, columns=["x"])
dfy = pd.DataFrame(y, columns=["y"])

df = pd.concat([dfX, dfy], axis=1)
```

**Plot 함수 새성**


```python
def plot_statsmodels(result, i, j):
    axs[i,j].scatter(X, y)
    
    xx = np.linspace(0, 1, 1000)
    dfxx = pd.DataFrame(xx, columns=["x"])
    y_hat = result.predict(dfxx).values
    
    axs[i,j].plot(xx, y_hat, "r-")
```

**각 방법별 회귀계수**


```python
# 모형 설정
model = sm.OLS.from_formula(
    "y ~ x + I(x**2) + I(x**3) + I(x**4) + I(x**5) + I(x**6) + I(x**7) + I(x**8) + I(x**9)", data=df)

# 기본 다항회귀모형
result1 = model.fit()

# Ridge 회귀모형 L1_wt=0
result2 = model.fit_regularized(alpha=0.01, L1_wt=0)

# Lasso 회귀모형 L1_wt=1
result3 = model.fit_regularized(alpha=0.01, L1_wt=1)

# Elastic Net 회귀모형 L1_wt: 0~1 사이 값
result4 = model.fit_regularized(alpha=0.01, L1_wt=0.5)

coef = pd.DataFrame()

coef["Basic"] = result1.params
coef["Ridge"] = result2.params
coef["Lasso"] = result3.params
coef["Elastic Net"] = result4.params
coef
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Basic</th>
      <th>Ridge</th>
      <th>Lasso</th>
      <th>Elastic Net</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>-0.169863</td>
      <td>0.633087</td>
      <td>0.687949</td>
      <td>0.656203</td>
    </tr>
    <tr>
      <th>x</th>
      <td>25.735773</td>
      <td>-0.757059</td>
      <td>-1.129134</td>
      <td>-0.849745</td>
    </tr>
    <tr>
      <th>I(x ** 2)</th>
      <td>-428.141684</td>
      <td>-1.070566</td>
      <td>-1.124878</td>
      <td>-1.262902</td>
    </tr>
    <tr>
      <th>I(x ** 3)</th>
      <td>3866.723115</td>
      <td>-0.768351</td>
      <td>0.000000</td>
      <td>-0.425687</td>
    </tr>
    <tr>
      <th>I(x ** 4)</th>
      <td>-18340.939665</td>
      <td>-0.355304</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>I(x ** 5)</th>
      <td>49326.072546</td>
      <td>0.012194</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>I(x ** 6)</th>
      <td>-78884.743074</td>
      <td>0.299178</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>I(x ** 7)</th>
      <td>74538.645153</td>
      <td>0.509692</td>
      <td>0.000000</td>
      <td>0.304049</td>
    </tr>
    <tr>
      <th>I(x ** 8)</th>
      <td>-38453.132190</td>
      <td>0.657937</td>
      <td>0.281484</td>
      <td>0.631908</td>
    </tr>
    <tr>
      <th>I(x ** 9)</th>
      <td>8350.254986</td>
      <td>0.758519</td>
      <td>1.075281</td>
      <td>0.801206</td>
    </tr>
  </tbody>
</table>
</div>



**각 방법별 회귀선**


```python
fig, axs = plt.subplots(2,2, figsize=(15,8))

plot_statsmodels(result1, 0, 0)
axs[0,0].set_title("Basic")

plot_statsmodels(result2, 0, 1)
axs[0,1].set_title("Ridge")

plot_statsmodels(result3, 1, 0)
axs[1,0].set_title("Lasso")

plot_statsmodels(result4, 1, 1)
axs[1,1].set_title("Elastic Net")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_301_0.png)
    


## 5.4.5 Scikit-Learn의 정규화 회귀모형

`sklearn` 패키지에서는 정규화 회귀모형을 위한 `Ridge`, `Lasso`, `ElasticNet` 이라는 별도의 클래스를 제공한다. 

**Ridge 회귀모형**
$$
\text{RSS} + \text{alpha} \sum \beta_i^2
$$

**Lasso 회귀모형**
$$
0.5 \times \text{RSS}/n + \text{alpha} \sum |\beta_i|
$$

**Elastic Net 회귀모형**
$$
0.5 \times \text{RSS}/n + \text{alpha} \times \big(0.5 \times  (1-\text{l1_ratio})\sum \beta_i^2 + \text{l1_ratio} \sum |\beta_i| \big)
$$

**Plot 함수 새성**


```python
def plot_sklearn(model, i, j):
    axs[i,j].scatter(X, y)
    
    xx = np.linspace(0, 1, 1000)
    y_hat = model.predict(xx[:, np.newaxis])
    axs[i,j].plot(xx, y_hat, "r-")
```

**각 방법별 회귀계수**


```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet

# 기본 다항회귀모형
poly = PolynomialFeatures(9,include_bias = False) # 절편 생성 False 나머지에서 기본옵션으로 생성
model = make_pipeline(poly, LinearRegression()).fit(X, y)
i1 = model.steps[1][1].intercept_

# Ridge 회귀모형
model2 = make_pipeline(poly, Ridge(alpha=0.01)).fit(X, y)
i2 = model2.steps[1][1].intercept_

# Lasso 회귀모형
model3 = make_pipeline(poly, Lasso(alpha=0.01)).fit(X, y)
i3 = model3.steps[1][1].intercept_

# Elastic Net 회귀모형
model4 = make_pipeline(poly, ElasticNet(alpha=0.01, l1_ratio=0.5)).fit(X, y)
i4 = model4.steps[1][1].intercept_

coef2 = pd.DataFrame()

coef2["Basic"] = model.steps[1][1].coef_
coef2["Ridge"] = model2.steps[1][1].coef_
coef2["Lasso"] = model3.steps[1][1].coef_
coef2["Elastic Net"] = model4.steps[1][1].coef_
coef2.loc["Intercept"] = [i1,i2,i3,i4]

coef2
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Basic</th>
      <th>Ridge</th>
      <th>Lasso</th>
      <th>Elastic Net</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>25.735773</td>
      <td>1.514304</td>
      <td>-0.076689</td>
      <td>-0.837680</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-428.141684</td>
      <td>-3.901608</td>
      <td>-2.440963</td>
      <td>-1.239446</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3866.723115</td>
      <td>-2.635986</td>
      <td>-0.000000</td>
      <td>-0.501667</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-18340.939669</td>
      <td>-0.415268</td>
      <td>-0.000000</td>
      <td>-0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>49326.072558</td>
      <td>1.075302</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-78884.743094</td>
      <td>1.649133</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>74538.645173</td>
      <td>1.520268</td>
      <td>0.000000</td>
      <td>0.303822</td>
    </tr>
    <tr>
      <th>7</th>
      <td>-38453.132201</td>
      <td>0.946514</td>
      <td>0.000000</td>
      <td>0.624592</td>
    </tr>
    <tr>
      <th>8</th>
      <td>8350.254988</td>
      <td>0.132996</td>
      <td>1.881887</td>
      <td>0.842629</td>
    </tr>
    <tr>
      <th>Intercept</th>
      <td>-0.169863</td>
      <td>0.494784</td>
      <td>0.565439</td>
      <td>0.665333</td>
    </tr>
  </tbody>
</table>
</div>



- `make_pipeline`을 이용해서 연속된 변환을 순차적으로 처리할 수 있다.

**각 방법별 회귀선**


```python
fig, axs = plt.subplots(2,2, figsize=(15,8))

plot_sklearn(model, 0, 0)
axs[0,0].set_title("Basic")

plot_sklearn(model2, 0, 1)
axs[0,1].set_title("Ridge")

plot_sklearn(model3, 1, 0)
axs[1,0].set_title("Lasso")

plot_sklearn(model4, 1, 1)
axs[1,1].set_title("Elastic Net")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_313_0.png)
    


## 5.4.5 Ridge 와 Lasso 모형의 차이

Ridge 모형은 회귀계수를 한꺼번에 축소시키는데 반해 Lasso 모형은 일부 가중치 계수가 먼저 0으로 수렴하는 특성이 있다.


```python
from sklearn.datasets import load_diabetes
diabetes = load_diabetes()
X = diabetes.data
y = diabetes.target
```

**$\alpha$ 변화에 따른 Ridge 회귀계수**


```python
alpha = np.logspace(-3, 1, 5)

df_ridge = pd.DataFrame()

for i, a in enumerate(alpha):
    ridge = Ridge(alpha=a).fit(X, y)
    df_ridge[f"{i}"] = np.hstack([ridge.intercept_, ridge.coef_])
    
df_ridge.columns = alpha
df_ridge
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0.001</th>
      <th>0.010</th>
      <th>0.100</th>
      <th>1.000</th>
      <th>10.000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>152.133484</td>
      <td>152.133484</td>
      <td>152.133484</td>
      <td>152.133484</td>
      <td>152.133484</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-9.551414</td>
      <td>-7.199457</td>
      <td>1.307349</td>
      <td>29.465746</td>
      <td>19.812822</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-239.090354</td>
      <td>-234.552930</td>
      <td>-207.194814</td>
      <td>-83.154885</td>
      <td>-0.918458</td>
    </tr>
    <tr>
      <th>3</th>
      <td>520.363367</td>
      <td>520.583136</td>
      <td>489.691080</td>
      <td>306.351627</td>
      <td>75.416167</td>
    </tr>
    <tr>
      <th>4</th>
      <td>323.828627</td>
      <td>320.523356</td>
      <td>301.769437</td>
      <td>201.629434</td>
      <td>55.025419</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-712.328205</td>
      <td>-380.607066</td>
      <td>-83.466074</td>
      <td>5.909369</td>
      <td>19.924600</td>
    </tr>
    <tr>
      <th>6</th>
      <td>413.383794</td>
      <td>150.483752</td>
      <td>-70.828096</td>
      <td>-29.515927</td>
      <td>13.948686</td>
    </tr>
    <tr>
      <th>7</th>
      <td>65.811629</td>
      <td>-78.591232</td>
      <td>-188.680164</td>
      <td>-152.040465</td>
      <td>-47.553816</td>
    </tr>
    <tr>
      <th>8</th>
      <td>167.513774</td>
      <td>130.313059</td>
      <td>115.712703</td>
      <td>117.311715</td>
      <td>48.259420</td>
    </tr>
    <tr>
      <th>9</th>
      <td>720.944468</td>
      <td>592.349587</td>
      <td>443.814054</td>
      <td>262.944995</td>
      <td>70.144068</td>
    </tr>
    <tr>
      <th>10</th>
      <td>68.122100</td>
      <td>71.133768</td>
      <td>86.748539</td>
      <td>111.878718</td>
      <td>44.213876</td>
    </tr>
  </tbody>
</table>
</div>



**$\alpha$ 변화에 따른 Lasso 회귀계수**


```python
alpha = np.logspace(-3, 1, 5)

df_lasso = pd.DataFrame()

for i, a in enumerate(alpha):
    lasso = Lasso(alpha=a).fit(X, y)
    df_lasso[f"{i}"] = np.hstack([lasso.intercept_, lasso.coef_])

df_lasso.columns = alpha
df_lasso
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0.001</th>
      <th>0.010</th>
      <th>0.100</th>
      <th>1.000</th>
      <th>10.000</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>152.133484</td>
      <td>152.133484</td>
      <td>152.133484</td>
      <td>152.133484</td>
      <td>152.133484</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-8.998449</td>
      <td>-1.306575</td>
      <td>-0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-238.899740</td>
      <td>-228.822331</td>
      <td>-155.362882</td>
      <td>-0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>520.261362</td>
      <td>525.560658</td>
      <td>517.182017</td>
      <td>367.701852</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>323.429484</td>
      <td>316.175320</td>
      <td>275.082351</td>
      <td>6.301904</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>5</th>
      <td>-720.251734</td>
      <td>-307.013677</td>
      <td>-52.540269</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>421.405141</td>
      <td>89.321688</td>
      <td>-0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>66.734168</td>
      <td>-105.081398</td>
      <td>-210.159753</td>
      <td>-0.000000</td>
      <td>-0.000000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>164.448873</td>
      <td>119.597989</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>725.340440</td>
      <td>571.330871</td>
      <td>483.914409</td>
      <td>307.605700</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>10</th>
      <td>67.475538</td>
      <td>65.007316</td>
      <td>33.672821</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
  </tbody>
</table>
</div>




```python
ax1 = plt.subplot(121)
plt.semilogx(df_ridge.T) # 데이터 프레임의 index를 기준으로 각 column의 변화를 그림
plt.xticks(alpha, labels=np.log10(alpha)) # x축은 alpha의 "승"으로 나타냄
plt.title("Ridge")

ax2 = plt.subplot(122)
plt.semilogx(df_lasso.T)
plt.xticks(alpha, labels=np.log10(alpha))
plt.title("Lasso")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_321_0.png)
    


- Ridge와 Lasso 회귀모형의 각 회귀계수가 alpha가 증가함에 따라 어떻게 변하는지 나타내었다.


- Ridge는 회귀계수들이 다같이 축소되고, Lasso는 일부변수가 먼저 0에 수렴하는 것을 알 수 있다.

## 5.4.6 최적 정규화

정규화에 사용되는 하이퍼 모수(hyper parameter) 등을 바꾸면 모형의 검증 성능이 달라진다. 

따라서 최적의 성능을 가져올 수 있는 정규화 하이퍼 모수를 선택하는 과정이 필요하다. 

이러한 과정을 최적 정규화(optimal regularization)라고 한다.

- 학습용 데이터를 사용한 성능은 정규화 가중치가 작으면 작을수록 좋아진다.(과최적화)


- 검증용 데이터를 사용한 성능은 정규화 가중치가 특정한 범위에 있을 때 가장 좋아진다.

**$\alpha$ 변화에 따른 Lasso 회귀모형 성능**


```python
from sklearn.datasets import load_boston
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error # 성능 평가 MSE
from sklearn.model_selection import cross_val_score

boston = load_boston()
X = boston.data
y = boston.target

# alpha: 0.0001 ~ 1
alphas = np.logspace(-4, 0, 200)

train_scores = []
test_scores = []

for a in alphas:
    model = Lasso(alpha = a)
    y_hat =model.fit(X, y).predict(X)
    
    # trian MSE
    train_score = -mean_squared_error(y, y_hat)
    
    # test MSE 평균 kfold=5
    # neg_mean_squared_error: -MSE
    test_score = np.mean(cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=5))
    
    train_scores.append(train_score)
    test_scores.append(test_score)

# test MSE 평균이 가장 작은 값, alpha
optimal_alpha = alphas[np.argmax(test_scores)] 
optimal_score = np.max(test_scores)

plt.plot(alphas, test_scores, "r-", label="Test 성능")
plt.plot(alphas, train_scores, "r--", label="Train 성능")

plt.axhline(optimal_score, linestyle=':')
plt.axvline(optimal_alpha, linestyle=':')
plt.plot(optimal_alpha, optimal_score, marker="o", c="black", ms=5, mew=5)

plt.title("최적 정규화")
plt.ylabel('성능')
plt.xlabel('정규화 가중치')
plt.legend()

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_327_0.png)
    


- 가중치 $\alpha$가 증가함에 따라 (제약이 강해질수록) Train의 성능은 감소한다.


- 가중치가 0일 땐 일반 선형회귀모형으로서 과최적화 되어있지만 제약이 강해질수록 Train의 성능은 감소 될 수 밖에 없다.

**검증성능 곡선**


```python
from sklearn.model_selection import validation_curve

# alpha: 0.0001 ~ 1
alphas = np.logspace(-4, 0, 200)

# Model.fit() 형태가 아닌 Model()을 넣어줌
train_scores, test_scores = validation_curve(
    Lasso(), X, y, "alpha", alphas, cv=5,
    scoring="neg_mean_squared_error")

plt.plot(alphas, test_scores.mean(axis=1), "r-", label="Test 성능 평균")
plt.plot(alphas, train_scores.mean(axis=1), "r--", label="Train 성능 평균")

plt.ylabel('성능')
plt.xlabel('정규화 가중치')
plt.legend()
plt.title("최적 정규화")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_330_0.png)
    


`validation_curve`를 사용하여 보다 쉽게 검증성능 곡선을 그릴 수 있다.


```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

n_samples = 100
np.random.seed(0)
X = np.sort(np.random.rand(n_samples))
y = np.sin(2 * np.pi * X) + np.random.randn(n_samples) * 0.5
X = X[:, np.newaxis]


model = Pipeline([("poly", PolynomialFeatures()),
                  ("lreg", LinearRegression())])

degrees = np.arange(1, 15)

# 파이프라인으로 만들어진 모형에서는 적용할 모형의 이름 문자열과 인수의 이름 문자열을 두 개의 밑줄로 연결
# "poly" + "__" + "degree"
train_scores, test_scores = validation_curve(
    model, X, y, "poly__degree", degrees, cv=100,
    scoring="neg_mean_squared_error")

plt.plot(degrees, test_scores.mean(axis=1), "o-", label="Test 성능 평균")
plt.plot(degrees, train_scores.mean(axis=1), "o--", label="Train 성능 평균")

plt.ylabel('성능')
plt.xlabel('다항 차수')
plt.legend()
plt.title("최적 정규화")

plt.show()
```


    
![png](/assets/images/post_images/2021-06-02-test/output_332_0.png)
    

