---
title: "[Python] 데이터 사이언스 스쿨 - 4.6 확률론적 선형 회귀모형"
excerpt: "4.6 확률론적 선형 회귀모형"
categories: 
  - dss
tags: 
    - Python
    - dss
    - 회귀분석
toc: TRUE
toc_sticky: TRUE
use_math: TRUE
---

[데이터 사이언스 스쿨](https://datascienceschool.net/intro.html) 자료를 토대로 공부한 내용입니다.

실습과정에서 필요에 따라 내용의 누락 및 추가, 수정사항이 있습니다.

---


**기본 세팅**


```python
import numpy as np
import pandas as pd

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
```


```python
%matplotlib inline
%config InlineBackend.figure_format = 'retina'

mpl.rc('font', family='NanumGothic') # 폰트 설정
mpl.rc('axes', unicode_minus=False) # 유니코드에서 음수 부호 설정

# 차트 스타일 설정
sns.set(font="NanumGothic", rc={"axes.unicode_minus":False}, style='darkgrid')
plt.rc("figure", figsize=(10,8))

warnings.filterwarnings("ignore")
```

# 4.6 확률론적 선형 회귀모형

## 4.6.1 모형구조

회귀분석에서 오차항과 $y$의 분포는 다음과 같다.

$$
\epsilon \sim \ N(0,\sigma^{2}) , \quad Y = X\beta + \epsilon
\;
\rightarrow Y \sim \ N(X\beta,\sigma^{2})
$$

그리고 추정회귀계수 $\hat{\beta}$ 의 분포는 다음과 같다.

$$\hat{\beta} \sim \ MN(\beta, \ \sigma^{2}(X^{T}X)^{-1})$$

$\hat{\beta}$ 의 분포는 아래와 같이 표준정규분포로 바꿔 줄 수 있다.

$$
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{\sigma^{2}((X^{T}X)^{-1})_{ii}}} \sim \ Z
$$

MSE(Mean Squared Error)는 아래와 같이 자유도가 $n-k$인 카이제곱분포를 따른다.

$$
\quad
\sqrt{\dfrac{(n-k)MSE}{\sigma^{2}}}
\ = 
\sqrt{\dfrac{RSS}{\sigma^{2}}} \sim \ \chi^{2}(n-k)
$$

표준정규분포 / 루트(카이제곱분포/자유도)는 T분포를 따르는 성질을 이용하면 $\hat{\beta}$ 의 분포는 다음과 같다.

$$
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{\sigma^{2}((X^{T}X)^{-1})_{ii}}} \div
\sqrt{\dfrac{MSE}{\sigma^{2}}} 
\ = 
\dfrac{\hat{\beta_{i}} - \beta_{i}}{\sqrt{MSE((X^{T}X)^{-1})_{ii}}}
\sim T_{(n-k)}
$$

위 설명을 다음과 같이 요약할 수 있다.

- 오차항은 정규성, 독립성, 등분산성등을 고려해 가정한다.


- 종속변수는 선형회귀모형으로서 모수인 회귀계수가 주어졌을 때 정규분포를 따르게 된다.


- 추정회귀계수도 종속변수의 선형결합 형태이므로 정규분포를 따르나 실제로 모분산을 알 수 없으므로 검정에선 MSE를 사용하여 T분포를 사용한다.

## 4.6.2 회귀계수의 검정


```python
from sklearn.datasets import load_boston
import statsmodels.api as sm

boston = load_boston()

df_boston = pd.DataFrame(boston.data, columns=boston.feature_names)
df_boston["MEDV"] = boston.target

# 카테고리 피처 풀랭크 방식
feature_names = list(boston.feature_names)
feature_names.remove("CHAS")
feature_names = [name for name in feature_names] + ["C(CHAS)"]

model2 = sm.OLS.from_formula("MEDV ~ 0 + " + "+".join(feature_names), data=df_boston)
result2 = model2.fit()

print(result2.summary())
```

                                OLS Regression Results                            
    ==============================================================================
    Dep. Variable:                   MEDV   R-squared:                       0.741
    Model:                            OLS   Adj. R-squared:                  0.734
    Method:                 Least Squares   F-statistic:                     108.1
    Date:                Fri, 11 Jun 2021   Prob (F-statistic):          6.72e-135
    Time:                        21:56:55   Log-Likelihood:                -1498.8
    No. Observations:                 506   AIC:                             3026.
    Df Residuals:                     492   BIC:                             3085.
    Df Model:                          13                                         
    Covariance Type:            nonrobust                                         
    ================================================================================
                       coef    std err          t      P>|t|      [0.025      0.975]
    --------------------------------------------------------------------------------
    C(CHAS)[0.0]    36.4595      5.103      7.144      0.000      26.432      46.487
    C(CHAS)[1.0]    39.1462      5.153      7.597      0.000      29.023      49.270
    CRIM            -0.1080      0.033     -3.287      0.001      -0.173      -0.043
    ZN               0.0464      0.014      3.382      0.001       0.019       0.073
    INDUS            0.0206      0.061      0.334      0.738      -0.100       0.141
    NOX            -17.7666      3.820     -4.651      0.000     -25.272     -10.262
    RM               3.8099      0.418      9.116      0.000       2.989       4.631
    AGE              0.0007      0.013      0.052      0.958      -0.025       0.027
    DIS             -1.4756      0.199     -7.398      0.000      -1.867      -1.084
    RAD              0.3060      0.066      4.613      0.000       0.176       0.436
    TAX             -0.0123      0.004     -3.280      0.001      -0.020      -0.005
    PTRATIO         -0.9527      0.131     -7.283      0.000      -1.210      -0.696
    B                0.0093      0.003      3.467      0.001       0.004       0.015
    LSTAT           -0.5248      0.051    -10.347      0.000      -0.624      -0.425
    ==============================================================================
    Omnibus:                      178.041   Durbin-Watson:                   1.078
    Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126
    Skew:                           1.521   Prob(JB):                    8.84e-171
    Kurtosis:                       8.281   Cond. No.                     2.01e+04
    ==============================================================================
    
    Notes:
    [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
    [2] The condition number is large, 2.01e+04. This might indicate that there are
    strong multicollinearity or other numerical problems.
    

- 기본적으로 회귀분석 결과에 각 회귀계수가 0인지에 대해 p-value 및 신뢰구간이 나타난다.

**단일계수 검정**


```python
print(result2.t_test("CRIM = -0.1"))
```

                                 Test for Constraints                             
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    c0            -0.1080      0.033     -0.244      0.808      -0.173      -0.043
    ==============================================================================
    

- p-value가 0.808로서 $H_{0}: \ CRIM = -0.1$을 채택한다.


- 이 외에도 신뢰구간에 포함되는 값을 기준으로 검정하면 모두 채택될 것이다.


```python
print(result2.t_test("C(CHAS)[0.0] = C(CHAS)[1.0]"))
```

                                 Test for Constraints                             
    ==============================================================================
                     coef    std err          t      P>|t|      [0.025      0.975]
    ------------------------------------------------------------------------------
    c0            -2.6867      0.862     -3.118      0.002      -4.380      -0.994
    ==============================================================================
    

- 현재 풀랭크 방식으로 CHAS=0 변수와 CHAS=1 변수를 사용하였다.


- CHAS가 0과 1일 때 차이가 있다는 것을 알 수 있다. 

**전체 회귀계수 검정**

- $H_{0}: \ \beta_{0} = \beta_{1} = \beta_{2} = ... = 0$


- 모든 회귀계수가 0이 아닌지(유의한지) 확인하는 것으로 이는 결정계수가 0인지에 대한 귀무가설과 동일하다.


- 회귀분석 결과의 F값과 p-value를 통해서 확인 할 수 있다.
